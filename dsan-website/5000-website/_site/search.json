[
  {
    "objectID": "bayes.html",
    "href": "bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "Naive Bayes classification is a machine learning algorithm that leverages Bayes’ theorem. It operates on the premise of conditional independence among features, making it “naive” in its assumptions. The goal of Naive Bayes is to categorize data into predefined classes or categories. Its nature is rooted in Bayes’ theorem, which entails prior probabilities, likelihoods, and posterior probabilities, all interplaying to determine the most likely class for a given set of features. There are different variants of Naive Bayes, such as Gaussian Naive Bayes for continuous numerical data, Multinomial Naive Bayes for text data, and Bernoulli Naive Bayes for binary features. For this project, I will be using Gaussian Naive Bayes and Multinomial Naive Bayes. I will be using Naive Bayes classification to build and assess models with the goal of predicting vertical jump outcomes using both record and text data. For the record data, I will be using NBA combine measurements, and for the text data, I will be using content from news articles generated and queried from the news API based on the player name. I will be looking at whether the data can predict whether or not a player has an above-average maximum vertical jump."
  },
  {
    "objectID": "bayes.html#record-data",
    "href": "bayes.html#record-data",
    "title": "Naïve Bayes",
    "section": "Record Data",
    "text": "Record Data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n\n\ncombine_df = pd.read_csv(\"../../data/01-modified-data/cleaned_NBA_combine.csv\")\n\n\ncombine_df = combine_df[combine_df[\"combine_year\"]&gt;2009]\ncombine_df = combine_df.dropna()\nmax_vert_mean = combine_df[\"MAX.VERTICAL\"].mean()\ncombine_df[\"above_max_vert_mean\"] = (combine_df[\"MAX.VERTICAL\"]&gt; max_vert_mean).astype(int)\n\n\nlabel_vec = combine_df[\"above_max_vert_mean\"]\ndrop_cols = [\"Unnamed: 0\", \"POS\", \"Name\", \"MAX.VERTICAL\", \"STANDING.VERTICAL\", \"MAX.TOUCH\", \"above_max_vert_mean\"]\n\nfeature_matrix = combine_df.drop(columns= drop_cols)\nfeature_cols = feature_matrix.columns.tolist()\n\n\n#Naive bayes\nclf = make_pipeline(StandardScaler(), svm.SVC())\nclf.fit(feature_matrix, label_vec)\n\n\nX = feature_matrix\ny = label_vec\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=5000\n)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nclf = GaussianNB()\nclf.fit(X_train_scaled, y_train)\n\nX_test_scaled = scaler.transform(X_test)\ntest_predictions = clf.predict(X_test_scaled)\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols)\nX_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols)\n\n\n#Performance Metrics\naccuracy_train = accuracy_score(y_train, clf.predict(X_train_scaled))\naccuracy_test = accuracy_score(y_test, test_predictions)\nprecision_train = precision_score(y_train, clf.predict(X_train_scaled))\nprecision_test = precision_score(y_test, test_predictions)\nrecall_train = recall_score(y_train, clf.predict(X_train_scaled))\nrecall_test = recall_score(y_test, test_predictions)\nf1_train = f1_score(y_train, clf.predict(X_train_scaled))\nf1_test = f1_score(y_test, test_predictions)\n\n#Bar chart\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\ntrain_results = [accuracy_train, precision_train, recall_train, f1_train]\ntest_results = [accuracy_test, precision_test, recall_test, f1_test]\n\nplt.figure(figsize=(10, 6))\nbar_width = 0.35\nindex = range(len(metrics))\nplt.bar(index, train_results, bar_width, label='Training')\nplt.bar([i + bar_width for i in index], test_results, bar_width, label='Test')\nplt.xlabel('Metrics')\nplt.ylabel('Scores')\nplt.title('Training vs. Test Results')\nplt.xticks([i + bar_width / 2 for i in index], metrics)\nplt.legend(loc='upper right')\nplt.show()\n\n#Calculate metrics\nconf_matrix = confusion_matrix(y_test, test_predictions)\nprecision = precision_score(y_test, test_predictions)\nrecall = recall_score(y_test, test_predictions)\naccuracy = accuracy_score(y_test, test_predictions)\ntn, fp, fn, tp = conf_matrix.ravel()\nspecificity = tn / (tn + fp)\nnpv = tn / (tn + fn)\nF1 = f1_score(y_test, test_predictions)\n\n#Graph confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Reds\", linewidths=0.5, cbar=False, square=True)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.xticks([0.5, 1.5], ['Predicted Negative', 'Predicted Positive'])\nplt.yticks([0.5, 1.5], ['True Negative', 'True Positive'])\nplt.show()\n\nprint('Precision:', precision) \nprint('Recall:', recall) \nprint('Specificity:', specificity) \nprint('Negative Predictive Value:', npv) \nprint('F1 Score:', F1)\nprint('Accuracy:', accuracy) \n\n#selectkbest feature selection\nk = 5\nselector = SelectKBest(score_func=f_classif, k=k)\nX_new = selector.fit_transform(feature_matrix, label_vec)\nselected_feature_indices = selector.get_support(indices=True)\n\nfeature_names = feature_matrix.columns.tolist()\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(selected_feature_indices)), X_new[0])\nplt.xlabel('Feature Names')  \nplt.ylabel('Feature Value')\nplt.title('Selected Features')\nplt.xticks(range(len(selected_feature_indices)), [feature_names[i] for i in selected_feature_indices], rotation=90)\nplt.show()\n\n\n\ndef fscore_from_k(k_val):\n  X_selector = SelectKBest(f_classif, k=k_val)\n  X_selector.fit(X_train_scaled_df, y_train)\n  top_k_colnames = list(X_selector.get_feature_names_out())\n  print(top_k_colnames)\n  X_train_new = X_train_scaled_df[top_k_colnames].copy()\n  X_test_new = X_test_scaled_df[top_k_colnames].copy()\n  clf_k = GaussianNB()\n  clf_k.fit(X_train_new, y_train)\n  y_pred_new = clf_k.predict(X_test_new)\n  score_k = f1_score(\n    y_true = y_test,\n    y_pred = y_pred_new\n  )\n  return score_k\n\nk_vals = list(range(1, 15))\nfscores = [fscore_from_k(kv) for kv in k_vals]\nfs_df = pd.DataFrame({'k': k_vals, 'f1_score': fscores})\n\n\nplot_obj = sns.lineplot(data=fs_df, x='k', y='f1_score', marker='o')\nplt.title(\"F1 Scores for Increasingly-Larger Subsets of Features\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nPrecision: 0.6923076923076923\nRecall: 0.8571428571428571\nSpecificity: 0.68\nNegative Predictive Value: 0.85\nF1 Score: 0.7659574468085107\nAccuracy: 0.7608695652173914\n['THREE.QUARTER.SPRINT']\n['BODY.FAT', 'THREE.QUARTER.SPRINT']\n['BODY.FAT', 'STANDING.REACH', 'THREE.QUARTER.SPRINT']\n['BODY.FAT', 'STANDING.REACH', 'LANE.AGILITY', 'THREE.QUARTER.SPRINT']\n['HEIGHT', 'BODY.FAT', 'STANDING.REACH', 'LANE.AGILITY', 'THREE.QUARTER.SPRINT']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'LANE.AGILITY', 'THREE.QUARTER.SPRINT']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'LANE.AGILITY', 'THREE.QUARTER.SPRINT']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT', 'BENCH.PRESS']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'HAND.LENGTH', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT', 'BENCH.PRESS']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'HAND.LENGTH', 'HAND.WIDTH', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT', 'BENCH.PRESS']\n['HEIGHT', 'WEIGHT', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'HAND.LENGTH', 'HAND.WIDTH', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT', 'BENCH.PRESS', 'STANDING.TOUCH']\n['HEIGHT', 'WEIGHT', 'BMI', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'HAND.LENGTH', 'HAND.WIDTH', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT', 'BENCH.PRESS', 'STANDING.TOUCH']\n['HEIGHT', 'WEIGHT', 'BMI', 'BODY.FAT', 'STANDING.REACH', 'WINGSPAN', 'HAND.LENGTH', 'HAND.WIDTH', 'LANE.AGILITY', 'SHUTTLE.RUN', 'THREE.QUARTER.SPRINT', 'BENCH.PRESS', 'WINGSPAN.HEIGHT.RATIO', 'STANDING.TOUCH']\n\n\n\n\n\n\n\n\n\nConclusions\nThe Naive Bayes classifiers for record data present good precision (0.6923) and high recall (0.8571), indicating its proficiency in accurately identifying positive outcomes. Specificity is at a reasonable 0.68, and the negative predictive value (NPV) is quite good at 0.85. The F1 score is relatively high at 0.7659, showcasing an overall strong predictive performance. The accuracy stands at 0.7609, reflecting good overall correctness. These scores do not strongly suggest overfitting or underfitting, reinforcing the presence of a well-balanced level of model complexity.\nThe bar chart shows that the model performed better on the test data than the training data, which is good since it shows the model is not overfitting. For feature selection, weight had the highest feature score at k=5, which is interesting considering the EDA results but does make sense. It also showed that the F1 score for the model was highest when using seven features. Overall, the results show that NBA combine measurements can be used to make a good ML model to predict maximum vertical jump."
  },
  {
    "objectID": "bayes.html#text-data",
    "href": "bayes.html#text-data",
    "title": "Naïve Bayes",
    "section": "Text Data",
    "text": "Text Data\n\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport joblib\nimport matplotlib.pyplot as plt\n\nX = joblib.load('text_matrix.pkl')\ntext_feature_names = joblib.load('text_names.pkl')\n\n#News api caps at 99 queries\nlabel_vec = combine_df[\"above_max_vert_mean\"].head(99)\n\n\nmax_document_freq = 0.4\nmin_document_count = 2\n\n\n#feature selection for text data\nk = 5\nselector = SelectKBest(score_func=chi2, k=k)\nX_new = selector.fit_transform(X, label_vec)\nselected_feature_indices = selector.get_support(indices=True)\nprint(\"Selected feature indices:\", selected_feature_indices)\nselected_feature_names = [text_feature_names[i] for i in selected_feature_indices]\nprint(\"Selected feature names:\", selected_feature_names)\n\n#Split data \nX_train, X_test, y_train, y_test = train_test_split(\n    X_new, label_vec,\n    test_size=0.2,\n    random_state=5000\n)\n\n#Scale features\nscaler = StandardScaler(with_mean=False)\nX_train_scaled = scaler.fit_transform(X_train)\nclf = MultinomialNB()\nclf.fit(X_train_scaled, y_train)\nX_test_scaled = scaler.transform(X_test)\ntest_predictions = clf.predict(X_test_scaled)\n\n#Performance metrics\naccuracy_train = accuracy_score(y_train, clf.predict(X_train_scaled))\naccuracy_test = accuracy_score(y_test, test_predictions)\nprecision_train = precision_score(y_train, clf.predict(X_train_scaled))\nprecision_test = precision_score(y_test, test_predictions)\nrecall_train = recall_score(y_train, clf.predict(X_train_scaled))\nrecall_test = recall_score(y_test, test_predictions)\nf1_train = f1_score(y_train, clf.predict(X_train_scaled))\nf1_test = f1_score(y_test, test_predictions)\n\n#Bar chart\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\ntrain_results = [accuracy_train, precision_train, recall_train, f1_train]\ntest_results = [accuracy_test, precision_test, recall_test, f1_test]\n\nplt.figure(figsize=(10, 6))\nbar_width = 0.35\nindex = range(len(metrics))\nplt.bar(index, train_results, bar_width, label='Training')\nplt.bar([i + bar_width for i in index], test_results, bar_width, label='Test')\nplt.xlabel('Metrics')\nplt.ylabel('Scores')\nplt.title('Training vs. Test Results')\nplt.xticks([i + bar_width / 2 for i in index], metrics)\nplt.legend(loc='upper right')\nplt.show()\n\n#Calculate metrics for confusion matrix\nconf_matrix = confusion_matrix(y_test, test_predictions)\nprecision = precision_score(y_test, test_predictions)\nrecall = recall_score(y_test, test_predictions)\naccuracy = accuracy_score(y_test, test_predictions)\ntn, fp, fn, tp = conf_matrix.ravel()\nspecificity = tn / (tn + fp)\nnpv = tn / (tn + fn)\nF1 = f1_score(y_test, test_predictions)\n\n\n\n#Calculate confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Reds\", linewidths=0.5, cbar=False, square=True)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.xticks([0.5, 1.5], ['Predicted Negative', 'Predicted Positive'])\nplt.yticks([0.5, 1.5], ['True Negative', 'True Positive'])\nplt.show()\n\n\n#Print metrics\nprint('Precision:', precision) \nprint('Recall:', recall) \nprint('Specificity:', specificity) \nprint('Negative Predictive Value:', npv) \nprint('F1 Score:', F1)\nprint('Accuracy:', accuracy) \n\n#Bar chart for selected features\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(selected_feature_indices)), selector.scores_[selected_feature_indices])\nplt.xlabel('Selected Features')\nplt.ylabel('Feature Score')\nplt.title('Selected Features')\nplt.xticks(range(len(selected_feature_indices)), selected_feature_names, rotation=90)\nplt.show()\n\n\n\nSelected feature indices: [ 839 2023 2478 3427 3841]\nSelected feature names: ['clipper', 'harden', 'knicks', 'porter', 'rocket']\nPrecision: 1.0\nRecall: 0.5\nSpecificity: 1.0\nNegative Predictive Value: 0.6666666666666666\nF1 Score: 0.6666666666666666\nAccuracy: 0.75\n\n\n\n\n\n\n\n\n\n\n\n\nConclusions\nThe Naive Bayes classifiers for the text data model exhibit good precision at 1.0, good specificity at 1.0, and pretty good accuracy 0.75. However, its recall is not great at 0.5, indicating room for improvement in correctly identifying positive outcomes. The negative predictive value (NPV) is also fair at 0.6667. Overall, the model’s performance is good in terms of precision, specificity, and accuracy, but there’s potential for enhancement in recall and NPV to achieve a more balanced performance. These scores indicate that the model is not exhibiting clear signs of overfitting or underfitting and has a balanced level of complexity.\nAgain, The bar chart shows that the model performed better on the test data than the training data, which is good since it shows the model is not overfitting. The top 5 selected features were “knick”, “harden”, “porter”, “clipper”, and “rocket”. These are all either the names of popular teams or the last name of a player. This possibly indicates connections to these specific teams, or players result in a higher likelihood to have a high vertical jump, but also could just be coincidental connections. Overall, the results show that text data can be used to make a somewhat accurate model to predict vertical jump, and reveal the words that happen to have the most connection to players with an above-average vertical jump."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Tyler McCormick",
    "section": "",
    "text": "About Tyler McCormick\n\nBio\nTyler McCormick is a senior at Georgetown’s McDonough School of Business pursuing a fifth-year Master’s in Data Science and Analytics. His work experience includes two early-stage startups, Nodar and Margot’s Morsels, supporting financial analysis and marketing and helping a large $2 billion consumer product and service company, Leaf Home, understand opportunities to significantly improve their gross margins. Majoring in operations and analytics, Tyler is building foundational business skills and expertise in helping businesses use data-driven decisions as a competitive advantage by leveraging data for real-time decision-making and long-term strategy. He is positioning himself on the cutting edge of how data, particularly AI, will change the future.\nTyler is also a Commission Member and Ambassador for the World Dunk Association, striving to create a more fair and objective scoring system for dunk contests with a mission to legitimize dunking as its own sport through organized competitions and to promote further education about the sport with the ultimate goal of making it an official Olympic sport. He is an avid dunker, enjoys weightlifting, and is a certified Community Emergency Response Team member and Ski Patrol.\nTyler is a graduate of Tahoe Expedition Academy, a new model of education in the mountains of Northern California that deploys expedition learning on real-world problem-solving. Students spend more than 30 days in the field each academic year to learn about issues such as climate change, systemic poverty, social justice, and immigration. The school is pioneering “constructive adversity” that involves pushing young people out of their comfort zones so they can learn and grow. Tyler has led and participated in field studies on US recidivism, climate change in the Olympia Valley, disaster relief from Hurricane Irma in Florida, and Haitian migration in the Dominican Republic.\n\n\nEducation\n\nGerogetown University\n\nMaster of Science in Data Science and Analytics\n\n\n\nGeorgetown University, McDonough School of Business\n\nBS, Major: Operations and Analytics, Minor: Entrepreneurship\n\n\n\nTahoe Expedition Academy"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "It does not take much evidence to show that sports and the sports industry have a massive influence both within the United States and throughout the world. The sports industry had a global revenue of 487 billion USD in 2022, according to statista.com. Strength is pivotal in athletic performance, serving as a fundamental component across various sports. While the degree of importance attributed to strength may vary from one sport to another, it remains a crucial factor in nearly every athletic endeavor. While skill and technique undeniably contribute to success, the prevailing belief is that stronger athletes often hold a significant competitive advantage and are typically considered better overall athletes. This perspective is reinforced by the stringent measures taken by many sports leagues, which ban and rigorously test for the use of performance-enhancing substances such as anabolic steroids. These regulations underscore the acknowledgment within the sports community that strength is a vital determinant of an athlete’s prowess.\nScientific studies further prove this point. A review article by Timothy J. Suchomel, Sophia Nimphius & Michael H. Stone titled, The Importance of Muscular Strength in Athletic Performance looks at previous literature that examines strength’s role in sports. The authors concluded that studies showed muscular strength highly correlated to better sprinting, jumping, changing direction, and sports performance. Not only this, but stronger individuals face a lower risk of injury. Evidence strongly suggests that improving muscular strength has no substitute for enhancing various attributes associated with athletic performance while reducing the risk of injury during these activities. For these reasons, athletes should aim to become as strong as possible within their sport’s context, as strength serves as a foundation for attributes that help improve athletic performance and reduce injury. (Suchomel, Nimphius, and Stone 2016)\nWith the established importance of strength, specifically in the context of sports, athletes are constantly looking for ways to improve their strength. This project will look at the factors that affect strength in athletes from a data science perspective. It will look at specific categories of factors such as training, nutrition, and recovery and what category of factors appear to have the greatest impact. The specific questions that will be addressed are:\n\nWhat is the most important factor that contributes to strength in athletes?\nHow does resistance training impact strength gains?\nWhat is the significance of periodization in designing strength and training programs?\nHow do different training modalities (e.g., weightlifting, powerlifting, Olympic lifting) affect specific aspects of strength?\nHow does nutrition, including macronutrient intake and timing of meals, influence an athlete’s strength?\nWhat role do supplements like creatine and caffeine play in enhancing strength performance?\nHow does hydration status affect an athlete’s ability to generate force?\nHow does adequate rest and sleep impact an athlete’s strength outputs?\nWhat recovery strategies, such as foam rolling and massage, contribute to maintaining optimal performance?\nHow can overtraining and fatigue negatively affect strength gains?\nWhat psychological techniques can enhance an athlete’s performance in maximal efforts?\nTo what extent do an athlete’s genetics influence their potential for developing strength?\nHow do age, gender, and body composition affect an individual’s strength capabilities?\n\nThere are a number of existing studies and literature that look at specific aspects of these questions. For example, another article by Timothy J. Suchomel, Sophia Nimphius, Christopher R. Bellon, & Michael H. Stone titled, The Importance of Muscular Strength: Training Considerations, looks at how different types of training affect strength. This article underscored that the development of muscular strength involves a multifaceted interplay of factors, including muscle hypertrophy, neuromuscular adaptations, and coordination. Diverse training modalities, such as resistance training or bodyweight exercises, influence these elements to facilitate strength gains. Moreover, strategic programming considerations, including factors like set volume and inter-set rest intervals, finely modulate the adaptive response. For individuals with a limited baseline strength, prioritizing hypertrophy and neuromuscular adaptation is prudent before emphasizing speed-related attributes. Conversely, those with substantial strength can enhance their performance by concentrating on speed-oriented training. (Suchomel et al. 2018)\nAnother review article and study by Olivier Dupuy, Wafa Douzi, Dimitri Theurot, Laurent Bosquet, and Benoit Dugué titled, An Evidence-Based Approach for Choosing Post-exercise Recovery Techniques to Reduce Markers of Muscle Damage, Soreness, Fatigue, and Inflammation: A Systematic Review With Meta-Analysis, identified various recovery techniques that can alleviate delayed onset muscle soreness and perceived fatigue after a single exercise session. This relates to strength, as recovery can affect strength gains and the ability for athletes to continue to train for strength. The article concludes that massage appears to be the most effective, followed by water immersion and compression garments. For managing perceived fatigue, compression techniques work well. Massage and cold exposure, like water immersion and cryotherapy, are effective to combat inflammation. However, the study ultimately concludes that more research is needed to assess these techniques with regular use and explore potential synergies between them for improved athlete performance. (Dupuy et al. 2018)\nBased on existing literature and personal experience, I would hypothesize that genetics, training, and nutrition contribute the most to strength in athletes. Specifically, weight training, proper protein, and micronutrient intake are the controllable factors that contribute to strength in athletes the most. However, I hope to have a more refined and evidence-based answer through data analysis in this study."
  },
  {
    "objectID": "index.html#factors-that-affect-strength-in-athletes",
    "href": "index.html#factors-that-affect-strength-in-athletes",
    "title": "Introduction",
    "section": "",
    "text": "It does not take much evidence to show that sports and the sports industry have a massive influence both within the United States and throughout the world. The sports industry had a global revenue of 487 billion USD in 2022, according to statista.com. Strength is pivotal in athletic performance, serving as a fundamental component across various sports. While the degree of importance attributed to strength may vary from one sport to another, it remains a crucial factor in nearly every athletic endeavor. While skill and technique undeniably contribute to success, the prevailing belief is that stronger athletes often hold a significant competitive advantage and are typically considered better overall athletes. This perspective is reinforced by the stringent measures taken by many sports leagues, which ban and rigorously test for the use of performance-enhancing substances such as anabolic steroids. These regulations underscore the acknowledgment within the sports community that strength is a vital determinant of an athlete’s prowess.\nScientific studies further prove this point. A review article by Timothy J. Suchomel, Sophia Nimphius & Michael H. Stone titled, The Importance of Muscular Strength in Athletic Performance looks at previous literature that examines strength’s role in sports. The authors concluded that studies showed muscular strength highly correlated to better sprinting, jumping, changing direction, and sports performance. Not only this, but stronger individuals face a lower risk of injury. Evidence strongly suggests that improving muscular strength has no substitute for enhancing various attributes associated with athletic performance while reducing the risk of injury during these activities. For these reasons, athletes should aim to become as strong as possible within their sport’s context, as strength serves as a foundation for attributes that help improve athletic performance and reduce injury. (Suchomel, Nimphius, and Stone 2016)\nWith the established importance of strength, specifically in the context of sports, athletes are constantly looking for ways to improve their strength. This project will look at the factors that affect strength in athletes from a data science perspective. It will look at specific categories of factors such as training, nutrition, and recovery and what category of factors appear to have the greatest impact. The specific questions that will be addressed are:\n\nWhat is the most important factor that contributes to strength in athletes?\nHow does resistance training impact strength gains?\nWhat is the significance of periodization in designing strength and training programs?\nHow do different training modalities (e.g., weightlifting, powerlifting, Olympic lifting) affect specific aspects of strength?\nHow does nutrition, including macronutrient intake and timing of meals, influence an athlete’s strength?\nWhat role do supplements like creatine and caffeine play in enhancing strength performance?\nHow does hydration status affect an athlete’s ability to generate force?\nHow does adequate rest and sleep impact an athlete’s strength outputs?\nWhat recovery strategies, such as foam rolling and massage, contribute to maintaining optimal performance?\nHow can overtraining and fatigue negatively affect strength gains?\nWhat psychological techniques can enhance an athlete’s performance in maximal efforts?\nTo what extent do an athlete’s genetics influence their potential for developing strength?\nHow do age, gender, and body composition affect an individual’s strength capabilities?\n\nThere are a number of existing studies and literature that look at specific aspects of these questions. For example, another article by Timothy J. Suchomel, Sophia Nimphius, Christopher R. Bellon, & Michael H. Stone titled, The Importance of Muscular Strength: Training Considerations, looks at how different types of training affect strength. This article underscored that the development of muscular strength involves a multifaceted interplay of factors, including muscle hypertrophy, neuromuscular adaptations, and coordination. Diverse training modalities, such as resistance training or bodyweight exercises, influence these elements to facilitate strength gains. Moreover, strategic programming considerations, including factors like set volume and inter-set rest intervals, finely modulate the adaptive response. For individuals with a limited baseline strength, prioritizing hypertrophy and neuromuscular adaptation is prudent before emphasizing speed-related attributes. Conversely, those with substantial strength can enhance their performance by concentrating on speed-oriented training. (Suchomel et al. 2018)\nAnother review article and study by Olivier Dupuy, Wafa Douzi, Dimitri Theurot, Laurent Bosquet, and Benoit Dugué titled, An Evidence-Based Approach for Choosing Post-exercise Recovery Techniques to Reduce Markers of Muscle Damage, Soreness, Fatigue, and Inflammation: A Systematic Review With Meta-Analysis, identified various recovery techniques that can alleviate delayed onset muscle soreness and perceived fatigue after a single exercise session. This relates to strength, as recovery can affect strength gains and the ability for athletes to continue to train for strength. The article concludes that massage appears to be the most effective, followed by water immersion and compression garments. For managing perceived fatigue, compression techniques work well. Massage and cold exposure, like water immersion and cryotherapy, are effective to combat inflammation. However, the study ultimately concludes that more research is needed to assess these techniques with regular use and explore potential synergies between them for improved athlete performance. (Dupuy et al. 2018)\nBased on existing literature and personal experience, I would hypothesize that genetics, training, and nutrition contribute the most to strength in athletes. Specifically, weight training, proper protein, and micronutrient intake are the controllable factors that contribute to strength in athletes the most. However, I hope to have a more refined and evidence-based answer through data analysis in this study."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this section I will be performing clustering using three different types of clustering. Clustering is an unsupervised learning technique that aims to discover groups or clusters within unlabeled data based on similarity or distance in a feature space. The main goal is to reveal the internal structure of the data, aiding in knowledge discovery through pattern identification and facilitating subset selection to manage large datasets. For this project I will be performing clustering on the NBA combine dataset without label features. Through this analysis, I hope to see if new insights can be revealed about the NBA combine event results and their relation to vertical jump through clustering."
  },
  {
    "objectID": "clustering/clustering.html#k-means-clustering",
    "href": "clustering/clustering.html#k-means-clustering",
    "title": "Clustering",
    "section": "K-Means CLustering",
    "text": "K-Means CLustering\nFor K-means clustering, I used the NBA combine dataset without label features as X. I normalized X, and tuned for the optimal number of clusters by using the elbow method. I did this by looping through values for the number of clusters, and plotted the resulting distortion and inertia. I then found the elbow point in the graph where both were minimized and increasing the number of clusters would produce diminishing lower values for inertia and distortion. I then plotted the clusters as colors on a scatter plot with ‘body fat’ and ‘three quarter sprint speed’ as the x and y axis. I chose to plot the clusters with these features since these two features were the two selected when I performed feature selection for the best two features in the Naive Bayes section.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.metrics import silhouette_score\n\ncombine_df = pd.read_csv(\"../../../data/01-modified-data/cleaned_NBA_combine.csv\")\ncombine_df = combine_df[combine_df[\"combine_year\"]&gt;2009]\ncombine_df = combine_df.dropna()\nmax_vert_mean = combine_df[\"MAX.VERTICAL\"].mean()\ncombine_df[\"above_max_vert_mean\"] = (combine_df[\"MAX.VERTICAL\"]&gt; max_vert_mean).astype(int)\n\nlabel_vec = combine_df[\"above_max_vert_mean\"]\ndrop_cols = [\"Unnamed: 0\", \"POS\", \"combine_year\", \"Name\"]\nfeature_matrix = combine_df.drop(columns= drop_cols)\ndb_matrix = feature_matrix[['BODY.FAT', 'THREE.QUARTER.SPRINT']]\nfeature_cols = feature_matrix.columns.tolist()\n\nX = feature_matrix\nscaler = StandardScaler()\nX_normalized = scaler.fit_transform(X)\n\ndb_X = db_matrix\nscaler = StandardScaler()\ndb_X_normalized = scaler.fit_transform(db_X)\n\n\n\n\nCode\nclusters = range(1,10)\nkmeans_list = []\ninertia_list = []\ndistortion_list = []\n\nfor k in clusters:\n    kmeans = KMeans(n_clusters=k, n_init=10)\n    kmeans.fit(X_normalized)\n    centroids = kmeans.cluster_centers_\n    labels = kmeans.labels_\n    \n\n    distortion = 0\n    for i in range(len(X_normalized)):\n        cluster_index = labels[i]\n        distortion += np.linalg.norm(X_normalized[i]-centroids[cluster_index])**2\n    distortion /= len(X_normalized)\n    distortion_list.append(distortion)\n    inertia = kmeans.inertia_\n    kmeans_list.append(k)\n    inertia_list.append(inertia)\n\ndf = pd.DataFrame({\"Clusters\": kmeans_list, \"Distortion\": distortion_list, \"Inertia\": inertia_list})\ndf.head(10)\n\n\n\n\n\n\n\n\n\nClusters\nDistortion\nInertia\n\n\n\n\n0\n1\n18.000000\n4086.000000\n\n\n1\n2\n13.396373\n3040.976684\n\n\n2\n3\n11.664875\n2647.926702\n\n\n3\n4\n10.489848\n2381.195597\n\n\n4\n5\n9.899263\n2247.132813\n\n\n5\n6\n9.424941\n2139.461719\n\n\n6\n7\n8.994443\n2041.738467\n\n\n7\n8\n8.582387\n1948.201760\n\n\n8\n9\n8.273089\n1877.991115\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1, 2)\nax[0].plot(df['Clusters'], df['Inertia'])\nax[0].set_xlabel('Number of Clusters')\nax[0].set_ylabel('Inertia')\nax[1].plot(df['Clusters'], df['Distortion'])\nax[1].set_xlabel('Number of Clusters')\nax[1].set_ylabel('Distortion')\n\n\nText(0, 0.5, 'Distortion')\n\n\n\n\n\n\n\nCode\noptimal_k = 5  \n\nkmeans = KMeans(n_clusters= optimal_k, n_init=10)\nkmeans.fit(X_normalized)\ny_pred = kmeans.predict(X_normalized)\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\n\nk_df = pd.DataFrame(X_normalized, columns=feature_cols)\nk_df['cluster_label'] = y_pred\nprint(k_df)\n\n\n\n       HEIGHT    WEIGHT       BMI  BODY.FAT  STANDING.REACH  WINGSPAN  \\\n0    0.769218 -0.124133 -1.043761 -1.416146        0.827347  0.440841   \n1   -1.130396 -1.386806 -0.901057 -0.750390       -1.627656 -1.774516   \n2    0.313311 -0.484897 -1.089426 -0.037080        0.009012 -0.065527   \n3   -1.282365 -1.292321 -0.552860 -0.797944       -1.013905 -0.825078   \n4    0.161342  0.408423  0.463190 -0.274850       -0.195571 -0.255414   \n..        ...       ...       ...       ...             ...       ...   \n222  0.541264  0.399833 -0.010586  0.723785        0.622763  0.694024   \n223 -0.142597  0.880852  1.593403  0.438461       -0.809322  0.694024   \n224 -0.066612  0.365475  0.680099  1.199325        0.213596 -0.065527   \n225  1.681032  0.451371 -1.197881  1.009109        1.543389  0.947208   \n226  1.681032  0.803545 -0.718397 -0.845498        0.622763  0.124361   \n\n     HAND.LENGTH  HAND.WIDTH  STANDING.VERTICAL  MAX.VERTICAL  LANE.AGILITY  \\\n0       0.592763    0.582597           0.240656      0.326839     -0.322871   \n1      -2.353744   -0.160378           2.081707      1.858015     -2.274532   \n2      -0.389406    0.582597           0.742761     -0.090755     -0.514552   \n3      -0.880490   -0.606163          -0.763553     -0.786744     -0.288020   \n4       0.101678    0.136812           0.408025      0.187641     -0.375148   \n..           ...         ...                ...           ...           ...   \n222    -1.371575    0.136812          -0.261448     -1.065139     -0.653957   \n223     1.083847    0.136812           1.579602      1.301224      0.635533   \n224    -1.371575   -0.903353          -0.428817     -1.065139      2.900853   \n225     0.592763    0.582597          -1.098289     -1.900326      2.116704   \n226    -0.389406    1.622762           1.914339      0.466037     -0.793361   \n\n     SHUTTLE.RUN  THREE.QUARTER.SPRINT  BENCH.PRESS  WINGSPAN.HEIGHT.RATIO  \\\n0      -0.941679             -0.063505    -0.951012              -0.430308   \n1      -1.867523             -1.576738    -0.330073              -1.438753   \n2       0.474318             -0.143148    -1.571951              -0.604178   \n3      -0.070296             -0.143148    -0.951012               0.578136   \n4       0.256473              0.653290     0.911805              -0.708500   \n..           ...                   ...          ...                    ...   \n222    -0.505988              0.812578    -0.744033               0.369492   \n223    -0.070296              0.334715     0.911805               1.447485   \n224     1.400163              2.007235    -0.744033              -0.013021   \n225     0.910010              1.529372     0.290866              -0.917144   \n226    -2.357676             -1.019231     1.739723              -2.273328   \n\n     STANDING.TOUCH  MAX.TOUCH  above_max_vert_mean  cluster_label  \n0          1.036246   1.204465             0.986870              3  \n1         -0.377906  -0.295905             0.986870              1  \n2          0.492342  -0.065079            -1.013304              2  \n3         -1.574497  -1.796274            -1.013304              4  \n4          0.057218  -0.065079             0.986870              3  \n..              ...        ...                  ...            ...  \n222        0.492342  -0.180492            -1.013304              2  \n223        0.165999   0.165747             0.986870              3  \n224       -0.051563  -0.642144            -1.013304              2  \n225        0.927466   0.165747            -1.013304              2  \n226        1.906494   1.089052             0.986870              3  \n\n[227 rows x 19 columns]\n\n\n\nimport seaborn as sns\n\nsns.scatterplot(data=k_df, x='BODY.FAT', y='THREE.QUARTER.SPRINT', hue = 'cluster_label', palette = 'muted')\n\n&lt;Axes: xlabel='BODY.FAT', ylabel='THREE.QUARTER.SPRINT'&gt;\n\n\n\n\n\n\nAnalysis\nThe optimal number of clusters for k-means was around 4-6 based on the elbow method. I chose to use 5 as the optimal number for my final k-means clustering. When plotted with ‘body fat’ and ‘three quarter sprint speed’ as the x and y axis, the clusters appear to overlap each other a lot. Each individual cluster and its shape can be made out and the area they span differ, but they overlap a lot, especially at one spot on the plot. Suggesting that k-means clustering is not able to greatly differentiate within the feature space."
  },
  {
    "objectID": "clustering/clustering.html#dbscan-clustering",
    "href": "clustering/clustering.html#dbscan-clustering",
    "title": "Clustering",
    "section": "DBSCAN Clustering",
    "text": "DBSCAN Clustering\nFor DBSCAN clustering, I used just the features, ‘body fat’ and ‘three-quarter sprint speed’. DBSCAN would not properly predict labels for the nba combine dataset if it contained more than two features, and those two features were the the two selected when I performed feature selection for the best two features. I then hyper-parameter tuned by using the elbow method for eps and minimum sample size based on silhouette score. I then graphed the resulting clusters with ‘body fat’ and ‘max vertical’ as the x and y axis, as I was interested to see if the clustering could provide any insights in that feature space, and the ‘body fat’ and ‘three-quarter sprint speed’ were the only 2 features used to perform clustering.\n\n\nCode\neps_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\nmin_samples_values = [1, 2, 3, 4, 5, 6]\n\n\nsilhouette_scores = []\neps_param = []\nmin_samp = []\n\n\nfor eps in eps_values:\n    for min_samples in min_samples_values:\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        labels = dbscan.fit_predict(db_X_normalized)\n\n        if len(set(labels)) &gt; 1:\n            silhouette = silhouette_score(db_X_normalized, labels)\n            silhouette_scores.append(silhouette)\n            eps_param.append(eps)\n            min_samp.append(min_samples)\n\nresult_df = pd.DataFrame({\"eps_value\": eps_param, \"min_samples\": min_samp, \"silhouette_score\": silhouette_scores})\n\nresult_df.head()\n\n\n\n\n\n\n\n\n\n\neps_value\nmin_samples\nsilhouette_score\n\n\n\n\n0\n0.1\n1\n0.196753\n\n\n1\n0.1\n2\n-0.066301\n\n\n2\n0.1\n3\n-0.184751\n\n\n3\n0.1\n4\n-0.336292\n\n\n4\n0.1\n5\n-0.385632\n\n\n\n\n\n\n\n\n\nCode\nmin_samples_values = [1, 2, 3, 4, 5, 6]\ngraph = sns.FacetGrid(result_df, col=\"min_samples\", col_wrap=3)\ngraph.map_dataframe(sns.lineplot, x=\"eps_value\", y=\"silhouette_score\", marker='o')\n\n\n\n\n\n\n\nCode\ndbscan = DBSCAN(eps=0.4, min_samples=5)\nlabels = dbscan.fit_predict(db_X_normalized)\n\ndbscan_df = pd.DataFrame(db_X_normalized, columns=['BODY.FAT','THREE.QUARTER.SPRINT'])\ndbscan_df['cluster_label'] = labels\ndbscan_df['MAX.VERTICAL'] = combine_df['MAX.VERTICAL']\n\nsns.scatterplot(data=dbscan_df, x='BODY.FAT', y='MAX.VERTICAL', hue = 'cluster_label', palette = 'muted')\n\n\n&lt;Axes: xlabel='BODY.FAT', ylabel='MAX.VERTICAL'&gt;\n\n\n\n\n\n\nAnalysis\nThe hyper-parameter tuning revealed the ideal parameters for eps and minimum sample size were 0.4 and 5 respectively. The plotted clustering with ‘body fat’ and ‘max vertical’ as the x and y axis did not have much pattern to it. There were only two cluster labels, a few of each. There was not much insight to be taken out other than the two selected features do not produce distinct clusters within the graphed feature space."
  },
  {
    "objectID": "clustering/clustering.html#hierarchical-clustering",
    "href": "clustering/clustering.html#hierarchical-clustering",
    "title": "Clustering",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nFor Hierarchical clustering, I used the NBA combine dataset without label features as X. I first created a dendrogram to visualize the clustering and select an optimal threshold to select the optimal number of clusters at a natural distance. I then plotted the clusters with ‘body fat’ and ‘three quarter sprint speed’ as the x and y axis.\n\n\nCode\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nlinkage_matrix = linkage(X_normalized, method=\"ward\")\ndendrogram(linkage_matrix)\n\nthreshold = 17\nplt.axhline(y=threshold, color='r', linestyle='-')\nplt.show()\n\n\n\n\n\n\n\nCode\ncluster = AgglomerativeClustering(n_clusters=4)\nlabels = cluster.fit_predict(X_normalized)\nagg_df = pd.DataFrame(X_normalized, columns=feature_cols)\nagg_df['cluster_label'] = labels\n\n\nsns.scatterplot(data=agg_df, x='BODY.FAT', y='THREE.QUARTER.SPRINT', hue = 'cluster_label', palette = 'muted')\n\n\n&lt;Axes: xlabel='BODY.FAT', ylabel='THREE.QUARTER.SPRINT'&gt;\n\n\n\n\n\n\nAnalysis\nThe dendrogram showed that a natural cutoff point was at 4 clusters, as the splits in the dendrogram became much more sporadic after. The resulting plot with cluster labels as colors the clusters once again appeared to overlap each other. The clusters were distinct in terms of the area and shape of each one, but there was a lot of overlap. Again suggesting that the clustering is not able to greatly differentiate when plotted in that feature space."
  },
  {
    "objectID": "clustering/clustering.html#conclusions",
    "href": "clustering/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions\nClustering the nba combine dataset using three different clustering algorithms in K-means, DBSCAN, and Hierarchical clustering and plotting for the features ‘body fat’ and ‘three-quarter sprint speed’, revealed a common theme of significant overlap among the clusters in the plotted feature space. This raises some questions about the distinctiveness of these features for effective clustering. For K-means, the optimal number of clusters, determined through the elbow method, was selected as 5. However, when visualizing the clusters in the feature space, significant overlap was observed, indicating that K-means struggled to differentiate within this space. Similarly, DBSCAN, after hyper-parameter tuning, did not reveal meaningful patterns in the plotted clusters, suggesting that the chosen features might not be conducive to distinctive clustering. Hierarchical clustering, with a natural cutoff point at 4 clusters, also exhibited overlapping clusters when visualized. These findings collectively suggest that the selected features may lack distinctiveness for effective clustering. Further exploration to find new distinct insights within the NBA combine dataset could be sought with alternative features or clustering techniques, however there may also not be further insights to be found."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In this section I will be attemping to provide insights on the project questions by performing exploratory data analysis on the datasets I gathered and cleaned. I will specifically be using the pandas and seaborn libraries for exploratory data analysis.\n\nNBA Player Combine and Season Data\n\nData overview\nThe NBA Player Combine and Season dataset contains a good number of features and datapoints. For this dataset, I am interested in looking at if there are physical and athletic predictors for vertical jump, and if vertical jump itself plays a role in determining on-court success in the NBA. The dataset consists of mostly continuous integer and float variable types, with a few categorical object type variables. There are a number of continous variables that indicate season statistics and on court-performance, as well as combine numbers that indicate physical and athletic attributes.\n\nimport pandas as pd\nimport numpy as np\n\nnba = pd.read_csv(\"../../../data/01-modified-data/cleaned_NBA_combined.csv\")\n\nprint('Shape:', nba.shape)\nprint('--------------------')\nprint('Feature and Data Information:')\nprint(nba.info())\n\n\nShape: (886, 67)\n--------------------\nFeature and Data Information:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 886 entries, 0 to 885\nData columns (total 67 columns):\n #   Column                         Non-Null Count  Dtype  \n---  ------                         --------------  -----  \n 0   Unnamed: 0                     886 non-null    int64  \n 1   Season                         886 non-null    int64  \n 2   Name                           886 non-null    object \n 3   Team                           886 non-null    object \n 4   Position                       886 non-null    object \n 5   Started                        886 non-null    int64  \n 6   Games                          886 non-null    int64  \n 7   FantasyPoints                  886 non-null    float64\n 8   Minutes                        886 non-null    int64  \n 9   FieldGoalsMade                 886 non-null    float64\n 10  FieldGoalsAttempted            886 non-null    float64\n 11  FieldGoalsPercentage           886 non-null    float64\n 12  EffectiveFieldGoalsPercentage  886 non-null    float64\n 13  TwoPointersMade                886 non-null    float64\n 14  TwoPointersAttempted           886 non-null    float64\n 15  TwoPointersPercentage          886 non-null    float64\n 16  ThreePointersMade              886 non-null    float64\n 17  ThreePointersAttempted         886 non-null    float64\n 18  ThreePointersPercentage        886 non-null    float64\n 19  FreeThrowsMade                 886 non-null    float64\n 20  FreeThrowsAttempted            886 non-null    float64\n 21  FreeThrowsPercentage           886 non-null    float64\n 22  OffensiveRebounds              886 non-null    float64\n 23  DefensiveRebounds              886 non-null    float64\n 24  Rebounds                       886 non-null    float64\n 25  OffensiveReboundsPercentage    886 non-null    float64\n 26  DefensiveReboundsPercentage    886 non-null    float64\n 27  TotalReboundsPercentage        886 non-null    float64\n 28  Assists                        886 non-null    float64\n 29  Steals                         886 non-null    float64\n 30  BlockedShots                   886 non-null    float64\n 31  Turnovers                      886 non-null    float64\n 32  PersonalFouls                  886 non-null    float64\n 33  Points                         886 non-null    float64\n 34  TrueShootingAttempts           886 non-null    float64\n 35  TrueShootingPercentage         886 non-null    float64\n 36  PlayerEfficiencyRating         886 non-null    float64\n 37  AssistsPercentage              886 non-null    float64\n 38  StealsPercentage               886 non-null    float64\n 39  BlocksPercentage               886 non-null    float64\n 40  TurnOversPercentage            886 non-null    float64\n 41  UsageRatePercentage            886 non-null    float64\n 42  PlusMinus                      886 non-null    float64\n 43  DoubleDoubles                  886 non-null    float64\n 44  TripleDoubles                  886 non-null    float64\n 45  PPG                            880 non-null    float64\n 46  RPG                            880 non-null    float64\n 47  APG                            880 non-null    float64\n 48  combine_year                   886 non-null    int64  \n 49  POS                            886 non-null    object \n 50  HEIGHT                         884 non-null    float64\n 51  WEIGHT                         882 non-null    float64\n 52  BMI                            882 non-null    float64\n 53  BODY.FAT                       818 non-null    float64\n 54  STANDING.REACH                 884 non-null    float64\n 55  WINGSPAN                       885 non-null    float64\n 56  HAND.LENGTH                    608 non-null    float64\n 57  HAND.WIDTH                     608 non-null    float64\n 58  STANDING.VERTICAL              766 non-null    float64\n 59  MAX.VERTICAL                   764 non-null    float64\n 60  LANE.AGILITY                   755 non-null    float64\n 61  SHUTTLE.RUN                    369 non-null    float64\n 62  THREE.QUARTER.SPRINT           761 non-null    float64\n 63  BENCH.PRESS                    571 non-null    float64\n 64  WINGSPAN.HEIGHT.RATIO          884 non-null    float64\n 65  STANDING.TOUCH                 766 non-null    float64\n 66  MAX.TOUCH                      764 non-null    float64\ndtypes: float64(57), int64(6), object(4)\nmemory usage: 463.9+ KB\nNone\n\n\n\nSummary stats for Variables of interest\nI chose to highlight the summary statistics for a few significant variables that are of interest. Standing Vertical and Maximum Vertical are important because they are the key metric that I am interested in looking at. While similar, the key difference is that for Maximum vertical jump players were allowed a running start, while they are not for standing vertical. Points per Game, Rebounds per Game, and Assists per Game were included as they represent the three most tracked statistics in basketball, and are often the three stats most used to represent success.\n\n\nprint(\"Standing Vertical Jump Summary Stats (inches)\") \nprint(nba[\"STANDING.VERTICAL\"].describe())\nprint(\"------------------------------------\")\n\nprint(\"Maximum Vertical Jump Summary Stats (inches)\")\nprint(nba[\"MAX.VERTICAL\"].describe())\nprint(\"------------------------------------\")\n\nprint(\"Points Per Game Summary Stats\")\nprint(nba[\"PPG\"].describe())\nprint(\"------------------------------------\")\n\nprint(\"Rebounds Per Game Summary Stats\")\nprint(nba[\"RPG\"].describe())\nprint(\"------------------------------------\")\n\nprint(\"Assists Per Game Summary Stats\")\nprint(nba[\"APG\"].describe())\nprint(\"------------------------------------\")\n\nStanding Vertical Jump Summary Stats (inches)\ncount    766.000000\nmean      29.695561\nstd        2.987566\nmin       21.500000\n25%       27.500000\n50%       29.500000\n75%       31.500000\nmax       41.500000\nName: STANDING.VERTICAL, dtype: float64\n------------------------------------\nMaximum Vertical Jump Summary Stats (inches)\ncount    764.000000\nmean      35.261780\nstd        3.538618\nmin       25.000000\n25%       33.000000\n50%       35.500000\n75%       37.500000\nmax       48.000000\nName: MAX.VERTICAL, dtype: float64\n------------------------------------\nPoints Per Game Summary Stats\ncount    880.000000\nmean      11.009432\nstd        9.256269\nmin        0.000000\n25%        3.700000\n50%        8.750000\n75%       16.100000\nmax       48.300000\nName: PPG, dtype: float64\n------------------------------------\nRebounds Per Game Summary Stats\ncount    880.000000\nmean       4.461250\nstd        3.743416\nmin        0.000000\n25%        1.800000\n50%        3.600000\n75%        5.925000\nmax       24.900000\nName: RPG, dtype: float64\n------------------------------------\nAssists Per Game Summary Stats\ncount    880.000000\nmean       2.321250\nstd        2.536934\nmin        0.000000\n25%        0.700000\n50%        1.500000\n75%        3.000000\nmax       17.100000\nName: APG, dtype: float64\n------------------------------------\n\n\n\n\n\nVisualizations\nFor visualizations I used boxplots and histograms to look at the distribution of Max Vertical Jump. I decided to look at max vertical jump specifically since it serves as aa better representation of an athletes highest possibly jumping ability and was so closely correlated to standing vertical jump, meaning standing vertical would likely produce similar resents when looking at the correlation between it and other variables. I looked at the correlation of maximum vertical jump to a number of variables to indicate in-game performance, as well as other combine metrics. For some visualizations I split by position to see if the results varied by position.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.boxplot(\n    data=nba,\n    x=\"Position\",\n    y=\"MAX.VERTICAL\",\n    order= [\"PG\", \"SG\", \"SF\", \"PF\", \"C\"]\n)\nsns.displot(\n    data=nba,\n    x=\"MAX.VERTICAL\"\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\",\n    y=\"PPG\",\n    hue=\"Position\",\n    col=\"Position\",\n    col_wrap=3\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\",\n    y=\"RPG\",\n    hue=\"Position\",\n    col=\"Position\",\n    col_wrap=3\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\",\n    y=\"APG\",\n    hue=\"Position\",\n    col=\"Position\",\n    col_wrap=3\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\",\n    y=\"BlockedShots\",\n    hue=\"Position\",\n    col=\"Position\",\n    col_wrap=3\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\",\n    y=\"PlusMinus\",\n    hue=\"Position\",\n    col=\"Position\",\n    col_wrap=3\n)\nsns.lmplot(\n    data=nba, \n    x=\"combine_year\",\n    y=\"MAX.VERTICAL\"\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"WEIGHT\", \n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"HEIGHT\", \n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"BMI\" \n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"BODY.FAT\", \n)\nsns.lmplot(\n    data=nba,\n    x=\"STANDING.VERTICAL\", \n    y=\"MAX.VERTICAL\", \n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"THREE.QUARTER.SPRINT\"\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"SHUTTLE.RUN\"\n)\nsns.lmplot(\n    data=nba,\n    x=\"MAX.VERTICAL\", \n    y=\"LANE.AGILITY\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Analysis\nI also made a correlation vector for just the combine metric variables to see what was the best physical and athletic predictors for vertical jump.\n\n\nnba_combine = pd.read_csv(\"../../../data/01-modified-data/cleaned_NBA_combine.csv\")\n\ncorrelation_vector = nba_combine.corrwith(nba_combine['MAX.VERTICAL'])\n\nprint(correlation_vector)\nsns.barplot(\n    x=correlation_vector,\n    y=correlation_vector.index   \n)\n\nplt.show()\n\nUnnamed: 0              -0.277047\ncombine_year             0.278785\nHEIGHT                  -0.396406\nWEIGHT                  -0.415299\nBMI                     -0.197752\nBODY.FAT                -0.495951\nSTANDING.REACH          -0.462754\nWINGSPAN                -0.277847\nHAND.LENGTH             -0.241322\nHAND.WIDTH              -0.125806\nSTANDING.VERTICAL        0.842795\nMAX.VERTICAL             1.000000\nLANE.AGILITY            -0.438503\nSHUTTLE.RUN             -0.222095\nTHREE.QUARTER.SPRINT    -0.599470\nBENCH.PRESS              0.040901\nWINGSPAN.HEIGHT.RATIO    0.143691\nSTANDING.TOUCH           0.080500\nMAX.TOUCH                0.325152\ndtype: float64\n\n\n/var/folders/sz/m07m37v53zbbxzbhqb8tjxjh0000gn/T/ipykernel_4025/2920671386.py:3: FutureWarning: The default value of numeric_only in DataFrame.corrwith is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_vector = nba_combine.corrwith(nba_combine['MAX.VERTICAL'])\n\n\n\n\n\n\n\nInsights\nThe range and median for maximum vertical jump tend to be higher for positions that typically have smaller players, indicating the importance of agility and leaping ability in these roles. There isn’t a strong correlation between points per game (PPG), rebounds per game (RPG), assists per game (APG), and PlusMinus(point differential for when a player is on the court) with maximum vertical jump. However, there is a very weak positive correlation for blocks, primarily for the center position, while other positions show minimal correlation. The distribution of maximum vertical jumps appears to be generally normal, with the peak around 35 inches. There is one data point that could be considered an outlier. The player that represents that data point is Keon Johnson, who tested a combined record 48-inch max vertical jump, which was multiple inches higher than the previous record. Despite this being an outlier, I chose not to discard this data point as it represents a legitimate and official test, and other athletes have tested a number that high in a non-combine setting. On the physical attributes front, weight, height, and BMI exhibit moderate to weak negative correlations. Body fat percentage, on the other hand, displays a moderate negative correlation. The average test result for maximum vertical jump is increasing slightly over time, but not a lot. There is a very strong positive correlation between standing vertical jump and maximum vertical jump, which makes a lot of intuitive sense, as a player who can jump relatively high from a static position would likely jump relatively high with a running start. Moreover, the three-quarter sprint has a strong negative correlation. The shuttle run shows a weak negative correlation, and the lane agility test reveals a moderate negative correlation. The correlation vector for correlation to maximum vertical jump shows that standing vertical, three-quarter sprint, and body fat percentage exhibit the strongest correlations with a player’s maximum vertical jump.\nAs a whole, the data, visualizations, and correlations reveal a few key insights. An NBA player’s ability to jump does not seem to have a major correlation to on-court performance as far as basic on-court performance statistics show. Shorter and lighter players tend to jump higher, as shown by the negative correlation between maximum vertical jump and height, weight, and body fat percentage, as well as the distributions by position. This makes sense, as the bigger, heavier, and more body fat a player has, the more force is needed to propel them high when jumping. In addition, athletic abilities like sprint speed, lateral quickness, and ability to quickly change direction translate to jumping higher, as shown by the correlation plots between maximum vertical jump and the three-quarter sprint, lane agility, and shuttle run drill times.\n\n\n\nNFL Combine Data\n\nData Overview\nThe NFL combine dataset contains 14 features, most of which are continuous and represent combine measurements. The NFL combine only has a standing vertical jump test, so I will be looking to compare the distribution and averages of the NFL combine results the results from the NBA combine.\n\nnfl = pd.read_csv(\"../../../data/01-modified-data/cleaned_NFL_combine.csv\")\n\nprint('Shape:', nfl.shape)\nprint('Feature and Data Information:')\nprint(nfl.info())\n\nShape: (4796, 15)\nFeature and Data Information:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 4796 entries, 0 to 4795\nData columns (total 15 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   Unnamed: 0         4796 non-null   int64  \n 1   Name               4796 non-null   object \n 2   Pos                4796 non-null   object \n 3   HEIGHT             4796 non-null   int64  \n 4   WEIGHT             4796 non-null   int64  \n 5   Forty              4751 non-null   float64\n 6   STANDING.VERTICAL  4796 non-null   float64\n 7   BenchReps          3595 non-null   float64\n 8   BroadJump          4658 non-null   float64\n 9   Cone               3936 non-null   float64\n 10  Shuttle            4002 non-null   float64\n 11  combine_year       4796 non-null   int64  \n 12  Team               2933 non-null   object \n 13  Round              2933 non-null   float64\n 14  Pick               2933 non-null   float64\ndtypes: float64(8), int64(4), object(3)\nmemory usage: 562.2+ KB\nNone\n\n\n\n\nSummary stats for Variables of interest\nThe main feature of interest in this dataset is the standing veritcal jump test, since it is essentially the same event that the NBA combine has, making it one of the few official and standardized points of athletic comparison for NFL and NBA players.\n\nprint(\"Standing Vertical Jump Summary Stats (inches)\") \nprint(nfl[\"STANDING.VERTICAL\"].describe())\nprint(\"------------------------------------\")\n\nStanding Vertical Jump Summary Stats (inches)\ncount    4796.000000\nmean       32.818599\nstd         4.213729\nmin        17.500000\n25%        30.000000\n50%        33.000000\n75%        35.500000\nmax        46.000000\nName: STANDING.VERTICAL, dtype: float64\n------------------------------------\n\n\n\n\nVisualizations for NBA combine compared to NFL combine\nFor visualizations I want to compare the distribution of vertical jump test results as well as if the correlation to physical attributes like height and weight differs between the two leagues. In order to look at this I will be using boxplots and scatterlots from a merged dataset that contains NBA and NFL combine results.\n\nnba_combine = pd.read_csv(\"../../../data/01-modified-data/cleaned_NBA_combine.csv\")\n\nnba_combine['league'] = \"NBA\"\nnfl['league'] = \"NFL\"\nnba_nfl = pd.concat([nfl,nba_combine], join='inner')\n\n\nsns.boxplot(\n    data=nba_nfl,\n    x=\"league\",\n    y=\"STANDING.VERTICAL\"\n)\n\nsns.lmplot(\n    data=nba_nfl,\n    x='combine_year',\n    y='STANDING.VERTICAL',\n    hue='league',\n    col='league',\n    fit_reg=False\n)\n\nsns.lmplot(\n    data=nba_nfl,\n    x='STANDING.VERTICAL',\n    y= 'WEIGHT',\n    hue='league',\n    col='league'\n)\n\nsns.lmplot(\n    data=nba_nfl,\n    x='STANDING.VERTICAL',\n    y= 'HEIGHT',\n    hue='league',\n    col='league'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\nIn analyzing the Exploratory Data analysis findings between the NFL and NBA, it becomes evident that the NFL exhibits a larger range, median, and maximum value in standing vertical jump compared to the NBA. Notably, when examining the correlation between weight and vertical jump performance in their respective combines, it is apparent that weight has a more pronounced negative correlation in the NFL combine. Height displays a weak negative correlation in both leagues, indicating that it may not significantly impact athletic success in either context. Vertical jump test performance does not significantly change over time in both the NFL and NBA.\nThese findings reveal a few key insights that make a lot of intuitive sense when looking at the nature of the respective sports. While basketball involves the actual action of jumping much more frequently than jumping, every play in football starts from a static position. So, it would make sense that, on average, a football player would be better at exerting power and force from a static position since they do so during every play. I would think that NBA players would be better at jumping with momentum, but any person who can jump well from a standing position would also likely jump high with a running start. And since the NFL does not have a maximum vertical jump test, we are unable to definitively say which players would be better at jumping with momentum, but I would imagine the NFL players would also have a slight edge in that. The greater range and more pronounced negative correlation to weight also makes a lot of sense because of the range of positions in the NFL. NBA players all play a relatively similar role on the court, and all positions require a well-balanced athlete. Positions in the NFL, on the other hand, vary much more drastically. As linemen require size and strength much more than “skill positions” like wide receivers, which favor quick and agile athletes. Overall, the results reflect the nature of the sport and the way they translate to the standing vertical jump test.\n\n\n\nOlympic High Jump Data\n\nData Overview\nThe Olympic high jump dataset has 9 features, most of which are categorical. Since Olympic high jump is measured much differently than the NBA and NFL combine, it is not useful to compare the numeric results, rather I am interested in looking at the distribution of the results, the change in results over time, and the distribution of nationalities in medaling place, as the olympics represent a large sample size of nationalities.\n\nhigh_jump = pd.read_csv(\"../../../data/01-modified-data/cleaned_high_jump.csv\")\n\nprint('Shape:', high_jump.shape)\nprint('Feature and Data Information:')\nprint(high_jump.info())\n\nShape: (107, 10)\nFeature and Data Information:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 107 entries, 0 to 106\nData columns (total 10 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Unnamed: 0       107 non-null    int64  \n 1   Gender           107 non-null    object \n 2   Event            107 non-null    object \n 3   Location         107 non-null    object \n 4   Year             107 non-null    int64  \n 5   Medal            107 non-null    object \n 6   Name             107 non-null    object \n 7   Nationality      107 non-null    object \n 8   Best Height (m)  104 non-null    float64\n 9   Unnamed: 8       0 non-null      float64\ndtypes: float64(2), int64(2), object(6)\nmemory usage: 8.5+ KB\nNone\n\n\n\n\nSummary stats for Variables of interest\n\n\nprint(\"Best Height (meters) Summary Stats\") \nprint(high_jump[\"Best Height (m)\"].describe())\nprint(\"------------------------------------\")\n\nBest Height (meters) Summary Stats\ncount    104.000000\nmean       2.023846\nstd        0.243316\nmin        1.600000\n25%        1.880000\n50%        2.010000\n75%        2.290000\nmax        2.390000\nName: Best Height (m), dtype: float64\n------------------------------------\n\n\n\n\nVisualizations\nFor visualizations I used histograms and scatterplots to look at the distribution of results and countries, as well as the results over time. Since the Olympic dataset contains the results from both the men’s and women’s events, I included both but seperated them.\n\nsns.displot(\n    data=high_jump,\n    x=\"Best Height (m)\",\n    hue = 'Gender',\n    col = 'Gender'\n)\n\nsns.lmplot(\n    data=high_jump,\n    x=\"Year\",\n    y=\"Best Height (m)\",\n    hue='Gender'\n    \n)\n\ncounts = high_jump[\"Nationality\"].value_counts()\nfiltered_high_jump = high_jump[high_jump[\"Nationality\"].isin(counts[counts &gt; 3].index)]\nsns.displot(\n    data=filtered_high_jump,\n    x=\"Nationality\",\n    hue = 'Gender',\n    col= 'Gender'\n)\n\n\n\n\n\n\n\n\n\n\n\n\nInsights\nEDA reveals that the distribution of the best height for both genders is not normal, but heavily left skewed. Additionally, it is evident that best height shows a robust positive correlation with time. In the men’s events, the United States outshines other countries with a significantly higher count of medal placements. In contrast, the women’s events exhibits a more equitable distribution of medals among several different nations.\nA few insights can be made from these results. While the NBA and NFL combine datasets do not reveal much change on average performance year after year, when looking at a dataset that represents a much longer timespan like 100 years of results, it becomes evident that athletes are continuing to perform better and better over time, just not at a rate which can be seen year to year. It also seems that at least for the Olympic event of high jump, men from the United States represent a huge number of medaling athletes. However I am very hesitant to make any conclusion that this means men from the US jump higher than those from other countries, since there are so many other factors that contribute to an olympic team’s success such as funding.\n\n\n\nStretching Data\n\nData Overview\nThe stretching dataset has 9 features, most of which are continuous. The data represents the results of a study that looked at the vertical jump height of a group of recreational athletes and collegiate students before and after doing dynamic stretching. The main interest with this dataset is to look at the range of improvement that occured among the participants.\n\nstretching = pd.read_csv(\"../../../data/01-modified-data/cleaned_stretching.csv\")\n\nprint('Shape:', stretching.shape)\nprint('Feature and Data Information:')\nprint(stretching.info())\n\nShape: (50, 10)\nFeature and Data Information:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50 entries, 0 to 49\nData columns (total 10 columns):\n #   Column                                 Non-Null Count  Dtype  \n---  ------                                 --------------  -----  \n 0   Unnamed: 0                             50 non-null     int64  \n 1   Participant number                     50 non-null     int64  \n 2   Group                                  50 non-null     int64  \n 3   Age                                    50 non-null     int64  \n 4   Gender                                 50 non-null     object \n 5   Height ( Cm )                          50 non-null     float64\n 6   Weight ( Kg )                          50 non-null     int64  \n 7   BMI                                    50 non-null     float64\n 8   Vertical jump\n height ( Pre ) ( Cm )   50 non-null     int64  \n 9   Vertical jump\n height ( Post ) ( Cm )  50 non-null     float64\ndtypes: float64(3), int64(6), object(1)\nmemory usage: 4.0+ KB\nNone\n\n\n\n\nSummary stats for Variables of interest\n\nprint(\"Pre dynamic stretching Vertical jump results (cm)\")\nprint(stretching[\"Vertical jump\\n height ( Pre ) ( Cm )\"].describe())\nprint(\"------------------------------------\")\n\nprint(\"Post dynamic stretching Vertical jump results (cm)\")\nprint(stretching[\"Vertical jump\\n height ( Post ) ( Cm )\"].describe())\nprint(\"------------------------------------\")\n\nPre dynamic stretching Vertical jump results (cm)\ncount    50.000000\nmean     30.900000\nstd       9.256548\nmin      15.000000\n25%      25.000000\n50%      28.500000\n75%      35.750000\nmax      49.000000\nName: Vertical jump\\n height ( Pre ) ( Cm ), dtype: float64\n------------------------------------\nPost dynamic stretching Vertical jump results (cm)\ncount    50.000000\nmean     33.950000\nstd      10.204166\nmin      16.000000\n25%      26.250000\n50%      32.500000\n75%      39.000000\nmax      57.000000\nName: Vertical jump\\n height ( Post ) ( Cm ), dtype: float64\n------------------------------------\n\n\n\n\nVisualizations\nFor visualizations I used boxplots and a histogram to look at the before and after results.\n\n\npre_post = stretching[[\"Vertical jump\\n height ( Pre ) ( Cm )\", \"Vertical jump\\n height ( Post ) ( Cm )\", 'Gender' ]]\nstretching['vert_difference'] = stretching[\"Vertical jump\\n height ( Post ) ( Cm )\"] - stretching[\"Vertical jump\\n height ( Pre ) ( Cm )\"]\nsns.boxplot(\n    data= pre_post\n)\n\nsns.displot(\n    data=stretching,\n    x='vert_difference'\n)\n\n\n\n\n\n\n\n\n\nInsights\nThe visualizations and summary stats show that the median and majority of participants saw an increase in vertical jump height following the dynamic stretching. The distribution of the distance is slightly skewed to the right with most seeing a small increase, a few seeing a decrease, and a few seeing a sizable increase in height. From this it can be inferred that warming up with movement and stretching for most people will likely have a positive effect on vertical jump height, or at least under the circumstances of the study.\n\n\n\nHypothesis Refinement and General insights\nBefore performing EDA, I originally hypothesized:\n\nSmaller and lighter players will tend to jump higher.\nJumping higher slightly correlates with better on court performance.\nAgility and speed will have positive correlation to vertical jump.\nNationality will not play a major role in vertical jump.\nAthletes are jumping higher over time.\nPlayers from other sports will have a similar vertical jump ability depending on the athleticism requiem in the sport.\nWarming up and stretching moderately improves vertical jump.\n\nAfter performing EDA and examining the results, I believe that, for the most part, my initial hypotheses were validated. Smaller and lighter players indeed tend to achieve higher vertical jumps, as evidenced by the negative correlation between weight and maximum vertical jump in both the NBA and NFL datasets. However, when it comes to on-court performance, there isn’t a strong correlation between maximum vertical jump and traditional statistics like points per game, rebounds, assists, and point differentials. In contrast, agility and speed tests, including the three-quarter sprint, lane agility, and shuttle run, show positive correlations with vertical jump, aligning with the hypothesis that these factors influence jumping ability. Furthermore, the data suggests that nationality does not significantly impact vertical jump performance, particularly evident in the Olympic high jump dataset. There is a slight increase in average maximum vertical jump results over time, but this is only made evident when looking at a large timespan rather than year to year. The impact of different sports on vertical jump ability and the influence of warming up through dynamic stretching are areas that require further exploration. Notably, the dynamic stretching study supports the notion that a warm-up involving movement and stretching can have a positive effect on vertical jump height for most participants."
  },
  {
    "objectID": "cleaning.html",
    "href": "cleaning.html",
    "title": "Tabular cleaning",
    "section": "",
    "text": "I cleaned this data with the intent to merge it with NBA player season data. The first thing I had to do with the NBA combine data was adjust the format of the player name column. I changed the format from (Last, First) to (First Last) using strsplit and sapply in R. I also changed the name of the column from “PLAYER” to “Name” so that can merge the dataset by “Name”. I then renamed some other columns to be more clear, and dropped those that were not needed. This dataset was relatively clean to begin with, so only a few minor adjustments were needed.\nLink to code\nLink to data"
  },
  {
    "objectID": "cleaning.html#text-cleaning-for-feature-selection",
    "href": "cleaning.html#text-cleaning-for-feature-selection",
    "title": "Tabular cleaning",
    "section": "Text cleaning for feature selection",
    "text": "Text cleaning for feature selection\n\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport joblib\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\nAPI_KEY='0e1b77cce9164a668886dca65fd25285'\n\n\n#Query\ndef fetch_data(TOPIC):\n    URLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n    response = requests.get(baseURL, URLpost) \n    response = response.json() \n    return response\n\n#commented out to avoid going over query limti\n#combine_df[\"uncleaned_news_text\"]= combine_df['Name'].apply(fetch_data)\n\ncombine_df.to_pickle(\"my_df.pkl\")\ncombine_df.to_csv(\"../../data/00-raw-data/raw_text.csv\")\n\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport joblib\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nimport joblib\nfrom tqdm import tqdm\ntqdm.pandas()\nimport string\nfrom collections import Counter\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n\ndf = pd.read_pickle(\"my_df.pkl\")\nsub_df = df.head(99)\n#cleaning funtion\ndef list_string_cleaner(input_list):\n    temp = []\n    for input_string in input_list:\n        try: \n            out=re.sub(r\"\"\"\n                        [,.;@#?!&$-]+  \n                        \\ *           \n                        \"\"\",\n                        \" \",          \n                        input_string, flags=re.VERBOSE)\n\n            out = re.sub('[’.]+', '', input_string)\n\n            out = re.sub(r'\\s+', ' ', out)\n\n            out=out.lower()\n        except:\n            print(\"ERROR\")\n            out=''\n        temp.append(out)\n\n    return temp\n\ndef extract_content(input):\n    cleaned_data = []\n    input_articles = input['articles']\n    for article in input_articles:\n\n        if 'content' in article:\n                cleaned_data.append(article['content'])\n\n    \n    return cleaned_data\n\ndef join_lists(list):\n    return \" \".join(list)\n\n\n#cleaning to strings\n\nsub_df[\"cleaned_news_text\"] = sub_df[\"uncleaned_news_text\"].apply(extract_content)\nprint(sub_df[\"cleaned_news_text\"].iloc[1])\nsub_df[\"cleaned_news_text\"] = sub_df[\"cleaned_news_text\"].apply(list_string_cleaner)\nsub_df[\"cleaned_news_text\"] = sub_df[\"cleaned_news_text\"].apply(join_lists)\n\n#string cleaning\n\nlemmatizer = WordNetLemmatizer()\ncustom_stopwords = [\n    'a', 'about', 'all', 'also', 'an', 'and', 'are', 'as', 'at',\n    'be', 'both', 'but', 'by', 'der', 'die', 'em', 'da', 'can',\n    'do',\n    'for', 'from',\n    'get', 'go',\n    'had', 'have',\n    'i', 'if', 'in', 'is', 'it',\n    'me', 'more', 'my',\n    'no', 'not',\n    'of', 'on', 'one', 'or', 'out',\n    'should', \"should've\", 'so',\n    'take', 'than', 'that', 'the', 'this', 'to', 'too',\n    'up',\n    'very',\n    'want', 'was', 'we', 'were', 'what', 'where', 'which', 'with', 'would', \"would've\",\n    'you', 'your',\n]\n\ncustom_stoplemmas = [\n    'be',\n    'ir',\n    'll',\n    'nt',\n    'quot',\n    'rd',\n    's',\n    've'\n]\n\n\ntoken_counter = Counter()\n\ndef remove_special_chars(token):\n  return token.translate(str.maketrans('', '', string.punctuation))\n\ndef remove_digits(token):\n  return ''.join([c for c in token if not c.isdigit()])\n\ndef clean(text):\n  cleaned = text.lower()\n  sents = sent_tokenize(cleaned)\n  clean_sents = []\n  # Tokenize each sentence\n  for cur_sent in sents:\n    sent_tokens = word_tokenize(cur_sent)\n    sent_tokens_cleaned = [t for t in sent_tokens if t not in custom_stopwords]\n    sent_tokens_cleaned = [remove_digits(t) for t in sent_tokens_cleaned]\n    sent_tokens_cleaned = [t.replace(\"-\", \" \") for t in sent_tokens_cleaned]\n    sent_tokens_cleaned = [remove_special_chars(t) for t in sent_tokens_cleaned]\n    sent_tokens_cleaned = [t for t in sent_tokens_cleaned if len(t) &gt; 0]\n    sent_tokens_cleaned = [lemmatizer.lemmatize(t) for t in sent_tokens_cleaned]\n    sent_tokens_cleaned = [t for t in sent_tokens_cleaned if t not in custom_stoplemmas]\n    token_counter.update(sent_tokens_cleaned)\n    clean_sent = ' '.join(sent_tokens_cleaned)\n    clean_sents.append(clean_sent)\n  final = \". \".join(clean_sents)\n  return final\n\n\nsub_df[\"cleaned_news_text\"] = sub_df[\"cleaned_news_text\"].apply(clean)\ntoken_counter.most_common(25)\n\n\n#vectorization\n\ncorpus = sub_df[\"cleaned_news_text\"].values\n\nmax_document_freq = 0.4\nmin_document_count = 2\n\ncv = CountVectorizer(max_df=max_document_freq, min_df=min_document_count)\nX = cv.fit_transform(corpus)\nX.shape\nfeature_names = cv.get_feature_names_out()\n\ncv_ng = CountVectorizer(max_df=max_document_freq, min_df=min_document_count)\nX_ng = cv_ng.fit_transform(corpus)\nX_ng.shape\n\njoblib.dump(feature_names, 'text_names.pkl')\njoblib.dump(X, 'text_matrix.pkl')\n\n/var/folders/sz/m07m37v53zbbxzbhqb8tjxjh0000gn/T/ipykernel_3831/4005829218.py:61: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub_df[\"cleaned_news_text\"] = sub_df[\"uncleaned_news_text\"].apply(extract_content)\n/var/folders/sz/m07m37v53zbbxzbhqb8tjxjh0000gn/T/ipykernel_3831/4005829218.py:63: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub_df[\"cleaned_news_text\"] = sub_df[\"cleaned_news_text\"].apply(list_string_cleaner)\n/var/folders/sz/m07m37v53zbbxzbhqb8tjxjh0000gn/T/ipykernel_3831/4005829218.py:64: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub_df[\"cleaned_news_text\"] = sub_df[\"cleaned_news_text\"].apply(join_lists)\n/var/folders/sz/m07m37v53zbbxzbhqb8tjxjh0000gn/T/ipykernel_3831/4005829218.py:129: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  sub_df[\"cleaned_news_text\"] = sub_df[\"cleaned_news_text\"].apply(clean)\n\n\n['This week saw MAGA Republicans engage in extremist arm wrestling in the House speaker race and the sins of the 2020 election subversion scheme begin to catch up with Donald Trumps closest allies. On … [+6676 chars]', 'Over the weekend, WorldCon announced the Hugo Award Winners in Chengdu, China, this years host city for one of the oldest genre conventions in the world. \\r\\nThis year was notable because Chengdu World… [+10058 chars]', 'TikTok tells us to do a lot of things in the name of beauty. A few examples: Reshape your teeth using a nail file; swap makeup primer for personal lubricant; slather yourself in beer before sitting o… [+3803 chars]', 'Universal\\xa0Destinations &amp; Experiences revealed today the name of the company’s first-ever permanent horror experience:\\xa0Universal\\xa0Horror Unleashed.\\u202f\\xa0\\r\\nThe new concept\\u202f– to be located at Area15 in L… [+2707 chars]', 'Chengdu, China Chengdu Worldcon, the 81st World Science Fiction Convention, has announced the winners of the 2023 Hugo Awards, Lodestar Award for Best Young Adult Book, and Astounding Award for Best … [+9587 chars]', 'Play: StreamsongLocated roughly 90 minutes southeast of Orlando is a hidden jewel of state-side links golf. Streamsong is a true destination, offering three Top 100-rated courses built on a former mi… [+6925 chars]', 'Is the role of a spinoff to stand on its own and branch off from the series from which it originated, or is it supposed to broaden its world and serve as a complement to it? The answer probably lies … [+7948 chars]', 'Having recently introduced environmentally friendly sneakers the likes of the Interact Run, Nike is still clearly making an effort to decrease their carbon footprint. And alongside implementing both … [+817 chars]', 'Les aventures de Rand, Nynaeve, Moiraine et les autres touchent pour le moment à leur fin, au terme dune saison 2 largement inégale. Mais si la série vous manque déjà, vous devriez trouver un certain… [+11176 chars]', 'Olympic 6s are back, this time they’re made suitable for the green. Dropping later on this month, here are official images of the Air Jordan 6 Low Golf Olympic.\\r\\nDone in the same fashion as the Air J… [+921 chars]', 'Bad to the bone.\\r\\nFour people have been accused of stealing $1 million in dinosaur bones from public land in Utah and selling and shipping the prehistoric findings to China, the DOJ revealed Thursday… [+3966 chars]', 'Bellingham scaling remarkable heights\\r\\nJude Bellingham started running deep in his own half and kept on going, his legs pumping and defenders trailing in his wake as the space opened up and the Wembl… [+7198 chars]', 'Nike’s Dunk Mid is unquestionably the “ugly duckling” of Peter Moore’s classic basketball sneaker from 1985. Recent memory, however, has seen the silhouette emerge in compelling styles, surely turnin… [+1026 chars]', 'Nike’s popular Dunk Low model surfaces yet again in a perfect colorway for those who can’t stay away from Travis Scott’s affinity for earth tones.\\r\\nAt a quick glance, it’s difficult to not to get hit… [+976 chars]', 'Beauty runs more than skin deep. Proper nutrition plays an important role in everything from skin health to hair growth to nail strength. A nutritious diet sets the tone for the overall health of our… [+4395 chars]', 'Arriving later on this year, here are official images of the Social Status x Nike Mac Attack in the Cobblestone colorway. This release is set to be the\\xa0third color option in their exclusive four-piec… [+1144 chars]', 'Currently slated to arrive next year, here are official images of the Nike Dunk Low Next Nature in the Midnight Navy and Team Gold colorway.\\r\\nLabeled as a women’s exclusive colorway of the model, Nex… [+974 chars]', 'ColorwayBlack/Particle Grey-Wolf Grey-Pink Rise\\r\\nRelease DateNovember 3, 2023\\r\\nWho’s excited for the debut of the Nike LeBron 21? Already previewed in a handful of colorways, today we give you an off… [+1200 chars]', 'Posted onOctober 24, 2023\\r\\n\\xa0\\xa0\\r\\nPosted byJohn Scalzi\\xa0\\xa0\\r\\n \\xa0\\r\\nLeave a Comment\\r\\nLots of things happened whilst I was on tour that I didn’t address because a) I was traveling, b) I was doing events, c) I … [+4175 chars]', 'After Rep. Kevin McCarthys ignominious removal from the House speakership last week, Rep. Steve Scalise has been pushed forward as the House GOPs nominee for speaker and the smiling face of Republica… [+5841 chars]', \"High-flying, adored ... (Image via Instagram)\\nABOVE: Some of these Instagram hardbodies have other lives — a pilot!\\n\\nBELOW: Keep reading for Matt's boner, Old Guys in Bed, Trump thumped and more \\xa0...… [+1183 chars]\", 'Reebok teams up with outdoor and active lifestyle brand Spyder to bring to you the limited edition Spyder x Reebok Collection. Previewed below, this capsule\\xa0leverages Reebok and Spyders iconic status… [+1744 chars]', 'Gen Vis in full swing with exploding penises, bronze crotches ripped open, and a supernova suicide that lets viewers know this season will have the highest of stakes from the very start. The Boys int… [+1718 chars]', 'Want some more The Simpsons x adidas collab? Say less and get a first look at the upcoming The Simpsons x adidas adiFOM Superstar Clouds.\\r\\nThis latest unveiled pair from the forthcoming The Simpsons … [+977 chars]', 'As a follow up to The Simpsons x adidas Forum Low Living Room collab we shared with you yesterday, we now give you a first look at another upcoming The Simpsons x adidas collaboration, The Simpsons x… [+925 chars]', \"Mary Trump has issued a warning about the new speaker of the House, Mike Johnson.\\r\\nOn Wednesday, House Republicans elected Johnson following weeks of turmoil over who would be the chamber's next lead… [+3426 chars]\", 'After making its debut last month in the “Dualism” colorway, the New Balance TWO WXY V4 has now surfaced in the all-new Atmosphere colorway. Now up for grabs, let’s see what this new New Balance bask… [+1635 chars]', 'This is an article version of the CBS Sports HQ AM Newsletter, the ultimate guide to every day in sports.\\xa0You can sign up to get it in your inbox every weekday morning here.\\r\\nGood morning to everyone… [+8621 chars]', 'If you were hoping to get your hands on some new adidas Yeezys this fall/holiday season well there’s been a change of plans as adidas has cancelled any adidas Yeezy releases indefinitely. According t… [+1204 chars]', 'A field crew studying fossil tracks near Lake Powell recently discovered an \"extremely rare\" set of prehistoric fossils along a stretch of the reservoir in Utah, officials announced on Friday. The cr… [+2919 chars]', 'Summary\\r\\n&lt;ul&gt;&lt;li&gt;\\r\\n Many of the best horror films of 2023 went unnoticed due to the dominance of popular franchises and the overwhelming release of Hollywood films. &lt;/li&gt;&lt;li&gt;\\r\\n Independent and foreig… [+9512 chars]', \"YOU MAY NOT know much about \\r\\nmagnesium, but it's really important you get enough of it. Getting an adequate amount of the nutrient is essential for your overall health. Magnesium is an unsung hero o… [+3105 chars]\", 'The House of Representatives is playing out a Ukraine dynamic that UK voters might recognize from Brexit. After Theresa May’s gambit of snap elections backfired, she was left with a small majority in… [+9866 chars]', \"Generally, exposing a young child to intentionally scary media isn't advisable. Its the sort of behavior of a babysitter with a taste for chaos, who knows theyll be long gone by the time the nightmar… [+15830 chars]\", 'PARK CITY, UT - JANUARY 24: Actress Maika Monroe from \"It Follows\" poses for a portrait at the ... [+] Village at the Lift Presented by McDonald\\'s McCafe during the 2015 Sundance Film Festival on Jan… [+14105 chars]', 'House Majority Leader Steve Scalise emerges from a House GOP conference meeting on Wednesday after his colleagues chose him to be their nominee for speaker.\\r\\nChip Somodevilla/Getty Images\\r\\nMajority L… [+6773 chars]', 'Four people have been indicted after allegedly purchasing and selling over $1 million worth of stolen dinosaur bones to China, according to federal authorities.\\r\\nBetween March 2018 and March 2023, th… [+3330 chars]', 'It started a few weeks ago. Friends and colleagues chatting over texts and direct messages, casually slotting it into conversation. Hey, is Wheel of Time good now? There is some exhaustion that inevi… [+5061 chars]', 'Over the course of the 2023/24 season we will hear from a wide range of local people who have benefitted from Arsenal in the Community projects. Here we find out more about how our latest intake on t… [+3953 chars]', \"The NFL trade deadline takes place on Oct. 31. Teams are nearly out of time to add significant outside talent this season, and with that in mind, FOX Sports' staff of NFL writers collaborated to come… [+12312 chars]\", \"The NFL trade deadline takes place on Oct. 31. Teams are nearly out of time to add significant outside talent this season, and with that in mind, FOX Sports' staff of NFL writers collaborated to come… [+12312 chars]\", \"As the MCU's Phase 5 continues with Loki Season 2's debut, and the DCU reboots itself under James Gunn, it's important to remember that they didn't always hold a monopoly on superhero movies. Or even… [+21994 chars]\", \"They like to say they're related by blood, but they've never met.\\r\\nUntil this weekend, that is.\\r\\nBack in 2012, Cody Jordan signed up for the bone marrow donation list in his hometown of Tupelo, Miss.… [+3261 chars]\", 'Thats right. Our entire global industrial civilization is going to collapse. And soon, which means within the lifetimes of most people alive today.\\r\\nI realize this is quite the claim, and a pretty te… [+64124 chars]', \"President Joe Biden makes his case for providing aid for Israel and Ukraine in a rare prime-time speech. The House's temporary speaker threatens to quit. And new research warns against dangers of ele… [+7771 chars]\", 'Teams across the NFL are hitting the practice field for the first time this week to gear up for Week 5. As is the case pretty much every week in the league, there are a number of injury situations th… [+10596 chars]', 'Welcome to Week 5 of the 2023 NFL season. As is the case pretty much every week, there are a number of injury situations that are worth monitoring. Friday, the 24 teams that play this Sunday will rev… [+10104 chars]', \"Here's a list of neglected but eminently useful words that visitors to this site -- and we, to be downright honest -- would like to bring back into fashion. You're right -- some never have been in fa… [+166437 chars]\", 'This weekend, the winners of the Hugo Awards, the Astounding Award for Best New Writer, and the Lodestar Award for Best Young Adult Book were announced at the 81st World Science Fiction Convention (W… [+8986 chars]', 'Federal prosecutors today announced charges against four people allegedly involved in a massive dinosaur bone smuggling scheme. Thousands of pounds of dinosaurs and other fossils were secretly excava… [+4820 chars]', '&lt;ul&gt;&lt;li&gt;The Springboks were crowned Rugby World Cup Champions on Saturday evening in France.&lt;/li&gt;&lt;li&gt;In an epic final, South African ran out 12-11 winners against the All Blacks.&lt;/li&gt;&lt;li&gt;They have no… [+4072 chars]', 'Populations of dogs and cats\\r\\nAs noted previously, AVMA [36] estimates of the US dog and cat populations were used, to provide the most conservative estimates of the impacts of dog and cat food. The … [+36858 chars]', 'Teams across the NFL are hitting the practice field for the first time this week to gear up for Week 5. As is the case pretty much every week in the league, there are a number of injury situations th… [+9043 chars]', 'CLAYTON A longtime Hillsdale employee was charged Wednesday after a Fox 2 photojournalist was run over in August with a public works truck.\\r\\nEarl D. Longmeyer, 57, was charged with driving in a carel… [+2558 chars]', 'Books &amp; the Arts / October 3, 2023\\r\\nThe Good Life\\r\\nWhat can we learn from the history of utopianism?\\r\\nWhat Can We Learn From the History of Utopianism? \\r\\nIn Everyday Utopias, Kristen Ghodsee look… [+13811 chars]', 'The screams can be heard from outside the house a raucous cacophony punctuated by a single, high-pitched shriek.\\r\\nIts 7 a.m. and Ive just made the nail-biting,hairpin turn-studded drive up Las Flores… [+12817 chars]', 'Ana Navarro has a bone to Pinkett with Jada.\\r\\n“The View” host held nothing back when addressing Jada Pinkett Smith’s recent announcement that she and\\xa0Will Smith\\xa0have actually been\\xa0secretly separated\\xa0… [+2596 chars]', 'Part of the Series\\r\\nIn retaliation against the Palestinians in Gaza for Hamass October 7 killing of hundreds of Israeli civilians, Israel has intensified its 16-year siege of Gaza to a complete siege… [+14923 chars]', 'Here\\'s what BuzzFeed Shopping writer\\xa0Jordan Grigsby has to say about this:\\xa0\\r\\n\"You see that shine? That volume? That bounce? Babyyyy my hair is FLAWLESS. Witchcraft, right? Oh, and if you could feel i… [+1275 chars]', 'The new speaker of the House of Representatives opened up about his Christian faith and how it informs his politics in an interview with The Daily Signal, in which he stressed the importance of treat… [+13419 chars]', 'The famed black metal stalwarts Krieg recently celebrated the release of their latest triumph, Ruiner. Of course, this American institution of musical integrity has proved once again that they remain… [+15586 chars]', \"The Rolling Stones - 'Hackney Diamonds' review: they've still got itFor a long while, the response to each new Rolling Stones announcement has been less about the music and more to do with their age.… [+5192 chars]\", 'New Zealand defended for a final 37-phases to help seal a narrow victory over Ireland in their quarter-final\\r\\n&lt;table&gt;&lt;tr&gt;&lt;th&gt;2023 Rugby World Cup semi-final: Argentina v New Zealand&lt;/th&gt;&lt;/tr&gt;\\r\\n&lt;tr&gt;&lt;t… [+5665 chars]', 'Week 5 got started with a bang for the Chicago Bears, who ended their 14-game losing streak with a 40-20 thumping of the Washington Commanders on Thursday night in Landover, Maryland.\\r\\nJustin Fields … [+6742 chars]', 'Skip to comments.\\r\\nLEAKED: Email From Lobbying Firm with Ties to RINO Mitch McConnell Organizing Phone Call with Jim Jordan Holdout Rep. Don Bacon on Eve of Speaker VoteThe Gateway Pundit ^\\r\\n | Octob… [+3310 chars]', 'Welcome to the Friday edition of the Pick Six newsletter.\\xa0\\r\\nThe NFL lost a legend on Thursday with the death of Dick Butkus, who passed away just hours before the Bears were set to take the field aga… [+15496 chars]', \"Welcome to the Thursday edition of the Pick Six newsletter!\\r\\nI'm going to start things off here by saying that if you see a Bears fan today, please hug them, because they probably need it. The Bears … [+15502 chars]\", 'The development of the earliest cities in Mesopotamia and the Middle East led to a substantial increase in violence between inhabitants. Laws, centralized administration, trade and culture then cause… [+1891 chars]', 'JONATHAN RICHMAN Backed by Tommy Larkins on drums, the former Modern Lovers leader brings his vast songbook and irrepressible charm which often causes his concerts to tip into the jubilant to Somervi… [+13812 chars]', 'I resoconti dalla Worldcon di Chengdu sono entusiastici: una sede spettacolare, allestita in modo altrettanto spettacolare, partecipazione del pubblico mai vista, con appassionati provenienti da ogni… [+8348 chars]', 'This article was originally published in 2015. The ranking has been updated to include\\xa0Killers of the Flower Moon, which is out in theaters now.\\r\\nLeonardo\\xa0DiCaprio\\xa0has been one of the biggest movie s… [+26891 chars]', 'An experimental replica of shell beads with Natufian red organic colorant made from the roots of Rubiaceae plants\\r\\n Laurent Davin, CC-BY 4.0\\r\\nThe flash of bright red caught Laurent Davins eye. He was… [+8875 chars]', \"The United Kingdom has rich culture, with many of the world's finest gardens, castles, and universities. At the same time, it nourishes a parallel history of criminal mayhem and loutishness probably … [+8363 chars]\", 'Many movies consist of a protagonist and antagonist working against one another, but its not always the villain who takes the main character down. Here are 24 movies where the main characters are the… [+8794 chars]', 'Halloween is the time of year when horror appeals to all ages and backgrounds, but especially so for the young and young at heart. Luckily, theres no shortage of family-friendly frights on streaming … [+12474 chars]', 'Halloween is the time of year when horror appeals to all ages and backgrounds, but especially so for the young and young at heart. Luckily, theres no shortage of family-friendly frights on streaming … [+12474 chars]', 'Midtowns lunchtime rush will soon have a new gluten-free contender.\\r\\nSpringbone Kitchen is opening its eighth location at 1155 Avenue of the Americas after signing a 15-year lease for 1,973 square fe… [+2640 chars]', \"Week 8 in the NFL season wasn't full of upsets like last week, but there were plenty of close games in the fourth quarter. 12 of 14 games (85.7%) have been within one score (eight points) in the four… [+19329 chars]\", 'Welcome to SNX DLX, your weekly roundup of the best sneakers to hit the internet. Today is going to be a tough week for Jordan fans, especially if your go-to is the Air Jordan 1. The famous silhouett… [+6621 chars]', 'STILL DOUBTS OVER GAZA CARNAGE\\r\\nUS President JoeBiden says the hospital blast that killed 471 Palestinians appears to have been “done by the other team” — that is, not Israel — the ABC reports. He la… [+12513 chars]', 'Nighty Night Has Received 3 Million Downloads Over 41 Episodes Since Its Debut\\r\\nLOS ANGELES, CA, Oct. 16, 2023 (GLOBE NEWSWIRE) -- via NewMediaWire -PodcastOne (NASDAQ: PODC), a leading podcast platf… [+7405 chars]', 'While documenting fossil tracksites along a stretch of Lake Powell, a Glen Canyon National Recreation Area (Glen Canyon NRA) field crew discovered the first tritylodontid bonebed found in the Navajo … [+4103 chars]', 'As an ardent devotee of the written word and an avid connoisseur of cinematic storytelling, I’ve always been enthralled by the experience of witnessing beloved fantasy novels come to life on the scre… [+10321 chars]', \"The Week 5 NFL schedule for the 2023 season is stacked with great matchups, and we've got you covered with what you need to know heading into the weekend. Our NFL Nation reporters bring us the bigges… [+31441 chars]\", 'Folks, there are many library systems you can turn to for your end-of-year Best Of lists when it comes to children’s books.\\r\\nNew York Public Library does one.\\r\\nChicago Public Library does one.\\r\\nAnd .… [+28488 chars]', 'Traveling can be a fun adventure, whether by yourself, with friends or family, or on a trip with your partner. Some people like their vacation packed with events and even use an itinerary to help pla… [+20041 chars]', 'On his bucket list was to be like Charlie Bucket.\\r\\nAnd sure enough, Joe Christianson plucked his version of a Willy Wonka golden ticket.\\r\\nAs the story goes, Christianson became fascinated by Jordan W… [+5841 chars]', 'The last time the Packers had a bye, they had just beaten the Bears, to improve to 5-8 and set the table for an unlikely (and ultimately failed) run at a playoff spot.\\r\\nThis year, their bye is coming… [+8620 chars]', 'Skip to comments.\\r\\nHamas Leader and Founding Member Khaled Mashal Calls for Global Muslim Uprising, Asking for Muslims Blood and Souls to be Sacrificed for Palestine This Upcoming Friday of Al-Aqsa F… [+4813 chars]', 'On his bucket list was to be like Charlie Bucket.\\r\\nAnd sure enough, Joe Christianson plucked his version of a Willy Wonka golden ticket.\\r\\nAs the story goes, Christianson became fascinated by Jordan W… [+5862 chars]', 'Hey, horror enthusiasts! Halloween is just around the corner. Let’s dive into some of the scariest horror movies that have ever haunted our dreams. These films have left an indelible mark on my psych… [+12461 chars]', 'Intertwined in president of baseball operations David Stearns search for new talent with the Mets will be deciding how to proceed with some of the old.\\r\\nThe Mets have 16 arbitration-eligible players,… [+7938 chars]', '&lt;table&gt;&lt;tr&gt;&lt;td&gt;\\r\\nREVENUE BY REGION\\r\\n&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;\\r\\n(in CHF million)\\r\\n&lt;/td&gt;&lt;td&gt;\\r\\nQ3 2023\\r\\n&lt;/td&gt;&lt;td&gt;\\r\\nQ3 2… [+11429 chars]', 'The start of the 2023 MLB Playoff season brought plenty of surprises, but that’s what the playoffs are about right? The top seeds in both the AL and the NL were sent home as the Baltimore Orioles wer… [+4294 chars]', 'The development of the earliest cities in Mesopotamia and the Middle East led to a substantial increase in violence between inhabitants. Laws, centralized administration, trade and culture then cause… [+3729 chars]', 'Maybe youre a Swiftie or perhaps youre only Swiftie-adjacent. Three hours of Taylor Swift: The Eras Tour will either be way too long for you or not nearly long enough. Undoubtedly, though, there are … [+9202 chars]', 'Abstract\\r\\nAlthough current evidence suggests increased risk of obesity, insulin resistance, and metabolic alterations in patients with polycystic ovary syndrome (PCOS), especially of a hyperandrogeni… [+37407 chars]', 'A woman fired several gunshots in the lobby of a Connecticut police station but bulletproof glass prevented anyone from being injured, authorities said Friday as they charged her with attempted murde… [+10699 chars]', 'As Shedeur Sanders continues to make national headlines with each game for the Colorado Buffaloes, many wonders whether the young quarterback will use this attention to enter the NFL Draft.\\xa0\\r\\nQuarter… [+5112 chars]', 'The recent surge in antisemitism worldwide following the Israel-Hamas war has sparked concern among Jewish and Israeli communities in the United States and Europe. Daily incidents of antisemitism hav… [+5947 chars]']\n\n\n['text_matrix.pkl']"
  },
  {
    "objectID": "gathering.html",
    "href": "gathering.html",
    "title": "Tabular Gathering",
    "section": "",
    "text": "NBA combine data\nThis data was gathered from Kaggle and represents the NBA draft combine testing results from the year 2000 to 2022. The NBA combine is an event in which NBA draft prospects are tested and measured on a number of physical and athletic attributes, including their standing vertical jump and their running vertical jump (max vertical jump). It is one of the few events in sports in which players’ physical attributes and athletic abilities are measured in an official and objective manner. I selected this dataset as it will be useful to measure whether there are other physical predictors for vertical jump, as well as for merging with season and game data to see whether vertical jump has an affect on in game performance.\nSource Link\nLink to dataset\n\n\n\nNBA player season data\nThis data was gathered from an API using R. The data represents NBA player data for each season from 2009 to 2023. It includes averages and totals from stats like points, rebounds, assists, and other game stats. I included this dataset as I plan to merge it with the previous NBA combine data to see if vertical jump has any relation to in game success for certain stats. I obtained a dataset with every players stats from each season by looping through years and querying the data from each season and then adding it to a single dataset.\nSource Link\nLink to dataset\nLink to code\n\n\n\nNFL combine data\nThis data was gathered from Kaggle and represents the NFL combine testing results from the year 2000 to 2018. The NFL combine is the same concept as the NBA combine, and is actually much more well known and covered than the NBA combine, as many positions in football rely heavily on physical attributes and abilities. I selected this dataset to compare the numbers to the NBA combine and compare how vertical jump for basketball players compares to other athletes, and the NFL is the only other league with a widely covered combine.\nSource Link\nLink to dataset\n\n\n\nOlympic Track and Field data\nThis data was also gathered from Kaggle and represents the results from all Olympic track & field events from 1896 to 2016. I plan to specifically look at the data from the high jump event. While the measurement system is different for high jumping, I would still like to examine how results have changed over time, and if any insights can be realized specifically with nationality.\nSource Link\nLink to dataset\n\n\n\nStretching Study data\nThis dataset comes from a study that looked at the the effect of dynamic stretching on vertical jump height in recreational athletes and collegiate students. It includes key metrics on the participants and their vertical results from before and after dynamic stretching. This will be useful to look at the effect that dynamic stretching has on vertical jump and if other factors with the participants play any role.\nSource Link\nLink to dataset\n\n\n\nText Gathering\n\nWikipedia API text data\nThis is text data from the Wikipedia page on basketball that was queried using the Wikipedia API. I included this data with the intention to look at what common words appear on the page and therefore are most associated with the sport of basketball, specifically words associated with physical attributes and skills. The code used to query the data is below.\nSource Link\nLink to datset\n\n\nimport wikipedia\n\nwikipedia.search(\"basketball\", results=10, suggestion=False)\n\n#wikipedia api swapping baseball and basketball for some reason\npage=wikipedia.page(title='baseball')\ntext = page.content\n\nwith open(\"wikipedia_text.txt\", 'w', encoding='utf-8') as file:\n    file.write(text)\n\n\n\nNews API text data\nThis text data was queried from the News API and containers articles with the word “basketball” in the description. I want to run a similar analysis with this as with the Wikipedia text data. The code used to query the data is below.\nSource Link\nLink to datset\n\nimport requests\nimport json\nimport re\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\nAPI_KEY='306919e989964d6ba9f61a0b153c64ba'\nTOPIC='Basketball'\n\n#Query\nURLpost = {'apiKey': API_KEY,\n            'q': '+'+TOPIC,\n            'sortBy': 'relevancy',\n            'totalRequests': 1}\n\n#request data from the server and extract txt data from request into json\nresponse = requests.get(baseURL, URLpost) \nresponse = response.json() \n\n\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y-%m-%d-H%H-M%M-S%S\")\n \nwith open(timestamp+'-newapi-raw-data.json', 'w') as outfile:\n    json.dump(response, outfile, indent=4)\n\njson_str = json.dumps(response)\n\nwith open(\"news_text.txt\", 'w') as file:\n    file.write(json_str)"
  },
  {
    "objectID": "dimensionality.html",
    "href": "dimensionality.html",
    "title": "Dimensionality Reduction",
    "section": "",
    "text": "In this section, I will be performing dimensionality reduction on the NBA combine dataset without label features. Dimensionality reduction is a tool used in data analysis and machine learning to address the challenges posed by high-dimensional datasets. Dimensionality reduction methods aim to capture the essential information contained in the original dataset while reducing the number of features. By transforming the data into a lower-dimensional representation, these techniques can enhance computational efficiency, alleviate the curse of dimensionality, and can reveal underlying patterns or structures in the data. I will be performing dimensionality reduction using PCA and t-SNE is python. PCA (Principal Component Analysis) is a statistical technique that reduces the dimensionality of high-dimensional datasets by identifying principal components through linear transformations, generating a low-dimensional representation useful for both supervised learning and data visualization. t-SNE (T-distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique that visualizes high-dimensional data in a lower-dimensional space by employing a probability distribution to group similar data points together while maintaining separation between dissimilar points. By performing dimensionality reduction using both techniques, I hope to gain insights on if combine measurements can be effectively predicted, retained, and shown with reduced dimensions, and if any future insights on other combine measurements can be related to vertical jump."
  },
  {
    "objectID": "dimensionality.html#dimensionality-reduction-with-pca",
    "href": "dimensionality.html#dimensionality-reduction-with-pca",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with PCA",
    "text": "Dimensionality Reduction with PCA\nFor Dimensionality Reduction with PCA, I first examined the eigenvalues of each feature to understand their individual contributions. Subsequently, I visualized the number of principal components against cumulative explained variance to determine the optimal number of components needed for a comprehensive representation of the data. Following this exploration, I performed PCA and created a graphical representation of the top two principal components, with ‘above_max_vert_mean’ assigned as the color variable.\n\n\nCode\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom numpy import linalg as LA\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.decomposition import PCA\n\ncombine_df = pd.read_csv(\"../../data/01-modified-data/cleaned_NBA_combine.csv\")\ncombine_df = combine_df[combine_df[\"combine_year\"]&gt;2009]\ncombine_df = combine_df.dropna()\nmax_vert_mean = combine_df[\"MAX.VERTICAL\"].mean()\ncombine_df[\"above_max_vert_mean\"] = (combine_df[\"MAX.VERTICAL\"]&gt; max_vert_mean).astype(int)\ncombine_df = combine_df.reset_index(drop=True)\n\n\nlabel_vec = combine_df[\"above_max_vert_mean\"]\ndrop_cols = [\"Unnamed: 0\", \"POS\", \"combine_year\", \"Name\", 'above_max_vert_mean']\nfeature_matrix = combine_df.drop(columns= drop_cols)\nfeature_cols = feature_matrix.columns.tolist()\n\n\n\n\n\nCode\ncov_matrix = feature_matrix.cov()\nw, v1 = LA.eig(cov_matrix)\n\nprint(\"\\nCOV EIGENVALUES:\")\neigenvalues = pd.Series(w, index=feature_cols)\nprint(eigenvalues)\n\n\n\n\n\n\nCOV EIGENVALUES:\nHEIGHT                   5.962825e+02\nWEIGHT                   3.982590e+01\nBMI                      3.156009e+01\nBODY.FAT                 1.208829e+01\nSTANDING.REACH           3.049225e+00\nWINGSPAN                 2.534890e+00\nHAND.LENGTH              2.255447e+00\nHAND.WIDTH               8.691960e-01\nSTANDING.VERTICAL        3.467110e-01\nMAX.VERTICAL             2.123330e-01\nLANE.AGILITY             7.489119e-02\nSHUTTLE.RUN              2.317702e-02\nTHREE.QUARTER.SPRINT     1.325364e-02\nBENCH.PRESS              6.936166e-03\nWINGSPAN.HEIGHT.RATIO    1.428815e-06\nSTANDING.TOUCH          -2.603432e-15\nMAX.TOUCH               -1.149081e-14\ndtype: float64\n\n\n\n\nCode\nX= feature_matrix\npca = PCA()\npca.fit(X)  \ncum_variance = pca.explained_variance_ratio_.cumsum()\n\nplt.plot(cum_variance, marker='o')\nplt.xlabel('Principal Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.show()\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nX= feature_matrix\nY= label_vec\n\n\nX=X/np.max(X) \n\n\nfrom sklearn.decomposition import PCA\nn_components=3\npca = PCA(n_components=n_components)\n\nX1=pca.fit_transform(X)\npca_df = pd.DataFrame(X1, columns=['x','y','z'])\npca_df['above_max_vert_mean'] = combine_df['above_max_vert_mean']\npca_df['above_max_vert_mean'] = pca_df['above_max_vert_mean'].map({1: 'yes', 0: 'no'})\nsns.scatterplot(data=pca_df, x='x', y='y', hue='above_max_vert_mean')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\n\n\n\n/Users/tyler/anaconda3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:84: FutureWarning:\n\nIn a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n\n\n\nText(0, 0.5, 'Principal Component 2')\n\n\n\n\n\n\nAnalysis\nFor the analysis PCA, it becomes apparent that ‘Height’ and ‘Weight’ exhibited the highest eigenvalues, suggesting their significance in capturing variance within the dataset. The graph illustrating the cumulative explained variance highlighted that the optimal number of principal components was three as it would sufficiently capture the majority of the dataset’s variability. Upon plotting the top two PCA components with ‘above_max_vert_mean’ as the color indicator, Principle Component 2 emerged as particularly informative. Notably, it effectively distinguished patterns based on the ‘above_max_vert_mean’ variable. Points (players) with a Principal Component 2 value of -0.1 or lower tended to exhibit an above-average max vertical jump, while those above this threshold generally had a below-average max vertical jump, with some overlap."
  },
  {
    "objectID": "dimensionality.html#dimensionality-reduction-with-t-sne",
    "href": "dimensionality.html#dimensionality-reduction-with-t-sne",
    "title": "Dimensionality Reduction",
    "section": "Dimensionality Reduction with t-SNE",
    "text": "Dimensionality Reduction with t-SNE\nFor Dimensionality Reduction with t-SNE, I leveraged Plotly for visualization, assigning the ‘above_max_vert_mean’ variable as the color and showcasing ‘max vert’ and ‘name’ as hover information. I systematically explored the impact of varying perplexity values, specifically examining settings at 1, 30, and 50.\n\n\nCode\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport plotly.express as px\nimport textwrap\nmywrap = lambda x: textwrap.wrap(x, width=60)\n\nX = feature_matrix\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=1).fit_transform(X)\ntsne_df = pd.DataFrame(X_embedded, columns=['x','y'])\ntsne_df['Name'] = combine_df['Name'] \ntsne_df['MAX.VERTICAL'] = combine_df['MAX.VERTICAL']\ntsne_df['above_max_vert_mean'] = combine_df['above_max_vert_mean']\ntsne_df['above_max_vert_mean'] = tsne_df['above_max_vert_mean'].map({1: 'yes', 0: 'no'})\ntsne_df['Name'] = tsne_df['Name'].apply(lambda x: x if type(x) == str else '')\ntsne_df['Name'] = tsne_df['Name'].apply(lambda x: '&lt;br&gt;'.join(mywrap(x)))\nprint(tsne_df)\nfig = px.scatter(tsne_df, x='x', y='y', hover_data=['Name', 'MAX.VERTICAL'], color = 'above_max_vert_mean')\nfig.update_layout(title_text='t-SNE with perplexity = 1')\nfig.show()\n\n\n             x          y               Name  MAX.VERTICAL above_max_vert_mean\n0   -15.465197  14.879143      Darius Bazley          37.0                 yes\n1   -12.802845  -0.848919        Jordan Bone          42.5                 yes\n2    19.057974 -10.679698     Brian Bowen II          35.5                  no\n3     0.229881 -49.098835          Ky Bowman          33.0                  no\n4    16.030859  47.432117   Ignas Brazdeikis          36.5                 yes\n..         ...        ...                ...           ...                 ...\n222   2.905999  16.738895  James Southerland          32.0                  no\n223  63.500896   0.325026      Adonis Thomas          40.5                 yes\n224   1.432116  15.660797     Deshaun Thomas          32.0                  no\n225  43.888191  41.715740        Jeff Withey          29.0                  no\n226 -20.255779 -58.241734        Cody Zeller          37.5                 yes\n\n[227 rows x 5 columns]\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nScreenshot of plottly graph, as it would not render in quarto \n\n\nCode\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport plotly.express as px\nimport textwrap\nmywrap = lambda x: textwrap.wrap(x, width=60)\n\nX = feature_matrix\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=30).fit_transform(X)\ntsne_df = pd.DataFrame(X_embedded, columns=['x','y'])\ntsne_df['Name'] = combine_df['Name'] \ntsne_df['MAX.VERTICAL'] = combine_df['MAX.VERTICAL']\ntsne_df['above_max_vert_mean'] = combine_df['above_max_vert_mean']\ntsne_df['above_max_vert_mean'] = tsne_df['above_max_vert_mean'].map({1: 'yes', 0: 'no'})\ntsne_df['Name'] = tsne_df['Name'].apply(lambda x: x if type(x) == str else '')\ntsne_df['Name'] = tsne_df['Name'].apply(lambda x: '&lt;br&gt;'.join(mywrap(x)))\nprint(tsne_df)\nfig = px.scatter(tsne_df, x='x', y='y', hover_data=['Name', 'MAX.VERTICAL'], color = 'above_max_vert_mean')\nfig.update_layout(title_text='t-SNE with perplexity = 30')\nfig.show()\n\n\n            x          y               Name  MAX.VERTICAL above_max_vert_mean\n0   -1.093742  -2.381273      Darius Bazley          37.0                 yes\n1   -8.359571  13.322024        Jordan Bone          42.5                 yes\n2   -5.752082   1.750604     Brian Bowen II          35.5                  no\n3   -4.980168  14.329522          Ky Bowman          33.0                  no\n4    3.972339  -7.520104   Ignas Brazdeikis          36.5                 yes\n..        ...        ...                ...           ...                 ...\n222  1.210846  -8.935663  James Southerland          32.0                  no\n223  6.964239 -11.026214      Adonis Thomas          40.5                 yes\n224  1.204727  -7.681103     Deshaun Thomas          32.0                  no\n225  1.046053 -11.420521        Jeff Withey          29.0                  no\n226  6.691407 -12.222388        Cody Zeller          37.5                 yes\n\n[227 rows x 5 columns]\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nScreenshot of plottly graph, as it would not render in quarto \n\n\nCode\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nimport plotly.express as px\nimport textwrap\nmywrap = lambda x: textwrap.wrap(x, width=60)\n\nX = feature_matrix\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=50).fit_transform(X)\ntsne_df = pd.DataFrame(X_embedded, columns=['x','y'])\ntsne_df['Name'] = combine_df['Name'] \ntsne_df['MAX.VERTICAL'] = combine_df['MAX.VERTICAL']\ntsne_df['above_max_vert_mean'] = combine_df['above_max_vert_mean']\ntsne_df['above_max_vert_mean'] = tsne_df['above_max_vert_mean'].map({1: 'yes', 0: 'no'})\ntsne_df['Name'] = tsne_df['Name'].apply(lambda x: x if type(x) == str else '')\ntsne_df['Name'] = tsne_df['Name'].apply(lambda x: '&lt;br&gt;'.join(mywrap(x)))\nprint(tsne_df)\nfig = px.scatter(tsne_df, x='x', y='y', hover_data=['Name', 'MAX.VERTICAL'], color = 'above_max_vert_mean')\nfig.update_layout(title_text='t-SNE with perplexity = 50')\nfig.show()\n\n\n            x         y               Name  MAX.VERTICAL above_max_vert_mean\n0   -2.220060 -0.818886      Darius Bazley          37.0                 yes\n1   -1.269619  9.427421        Jordan Bone          42.5                 yes\n2    0.451856  2.982492     Brian Bowen II          35.5                  no\n3    0.662682  9.162947          Ky Bowman          33.0                  no\n4    0.986679 -4.249359   Ignas Brazdeikis          36.5                 yes\n..        ...       ...                ...           ...                 ...\n222 -0.605657 -4.003376  James Southerland          32.0                  no\n223 -1.130381 -7.218080      Adonis Thomas          40.5                 yes\n224 -0.011604 -3.328646     Deshaun Thomas          32.0                  no\n225 -1.473130 -4.921151        Jeff Withey          29.0                  no\n226 -1.336228 -7.614920        Cody Zeller          37.5                 yes\n\n[227 rows x 5 columns]\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nScreenshot of plottly graph, as it would not render in quarto \n\nAnalysis\nAt a low perplexity (1), the t-SNE embeddings revealed distinct clusters, particularly 3-4 players with above-average vertical jumps forming isolated groups, while players with below-average vertical jumps clustered separately. Intriguingly, numerous clusters comprised players with identical ‘above_max_vert_mean’ values. Shifting to perplexity 30, the resulting distributions and clusters displayed variability with each code execution. Notably, it seemed that one feature often captured a player’s general size and build, while the other tended to represent their jumping ability, as discerned from both graph color and player names. Extending this analysis to perplexity 50, a similar pattern persisted, albeit with data points more evenly dispersed across the plot, contributing to a nuanced understanding of the dataset’s structure and relationships.\n\n\nComparison and Conclusion\nBoth PCA and t-SNE were able to capture combine event results and plot players based on size and athleticism with lower dimensions. T-SNE, visualized with Plotly, revealed distinct clusters at varying perplexity values, effectively capturing nuanced relationships between players’ size, build, and jumping ability. PCA, emphasizing ‘Height’ and ‘Weight’ through eigenvalues, identified three optimal components. Principal Component 2, when plotted against ‘above_max_vert_mean,’ effectively distinguished patterns, revealing its pivotal role in delineating jump performance. There is a general trade-off between the two techniques. While t-SNE provides high-quality visualizations, it is computationally expensive and sensitive to hyperparameters, potentially leading to varied results. But with PCA being a linear method, it may struggle to capture non-linear relationships in the data. But at least for this analysis, t-SNE and PCA complemented each other well, with PCA adept at distinguishing players based on their vertical jump ability and t-SNE excelling in capturing both jumping ability and size, especially at a higher perplexity."
  }
]